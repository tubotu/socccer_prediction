{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "split_num = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(data:pandas.core.frame.DataFrame, label:str) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract only the data of label of second argument from the inputted data.\n",
    "\n",
    "    @param data: the inputted data\n",
    "    @label: label you want to extract\n",
    "    @return: the extracted data\n",
    "    \"\"\"\n",
    "    return data[data.label == label]\n",
    "\n",
    "\n",
    "def data_column_conversion(data:pandas.core.frame.DataFrame, label:str) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract only the data of label of second argument from the inputted data.\n",
    "\n",
    "    @param data: the extracted data\n",
    "    @label: label you want to converse\n",
    "    @return: the conversed data\n",
    "    \"\"\"\n",
    "    data = data.assign(W = (label == 'W') + 0,D = (label == 'D') + 0,L = (label == 'L') + 0)\n",
    "    data = data.drop(\"label\",axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_randomization(data:pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomize data.\n",
    "\n",
    "    @param data: the conversed data you want to separate\n",
    "    @return: the randomized data\n",
    "    \"\"\"\n",
    "    return data.sample(n = len(data))\n",
    "\n",
    "\n",
    "def data_separate(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Separate data.\n",
    "    \n",
    "    @param data: the data you want to separate\n",
    "    @param split_num: the division number\n",
    "    \"\"\"\n",
    "    data_separate = []\n",
    "    for i in range(split_num):\n",
    "        data_separate.append(data[i::split_num])\n",
    "    return data_separate\n",
    "\n",
    "\n",
    "def divided_data_making_for_stratified_sampling(data:pandas.core.frame.DataFrame, split_num:int, label:str) -> list:\n",
    "    data = data_column_conversion(data,label)\n",
    "    data = data_randomization(data)\n",
    "    separated_data_list = data_separate(data, split_num)\n",
    "    return separated_data_list\n",
    "\n",
    "\n",
    "def data_list_wdl_merge(data_list1:list, data_list2:list, data_list3:list)-> list:\n",
    "    list_size = len(data_list1)\n",
    "    merged_data_list = []\n",
    "    for i in range(list_size):\n",
    "        merged_data_list.append(pd.concat([data_list1[i],data_list2[i],data_list3[i]]))\n",
    "    return merged_data_list\n",
    "\n",
    "\n",
    "def assign_group_numbers_to_data(data_list:list) -> list:\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i].assign(separate_num=i)\n",
    "    print(data_list[i]['separate_num'])\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_list_put_together(data_list:list) -> pandas.core.frame.DataFrame:\n",
    "    list_size = len(data_list)\n",
    "    data = data_list[0]\n",
    "    for i in range(1,list_size):\n",
    "        data = data.append(data_list[i])\n",
    "    return data\n",
    "\n",
    "\n",
    "def making_dataset_list_train(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    train_data_list = []\n",
    "    for i in range(split_num):\n",
    "        train_data_list.append(data[data['separate_num'] != i])\n",
    "    for i in range(split_num):\n",
    "        train_data_list[i] = train_data_list[i].drop(['separate_num'], axis = 1)\n",
    "        train_data_list = data_randomization(train_data_list[i])\n",
    "    return train_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_val(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    val_data_list = []\n",
    "    for i in range(split_num):\n",
    "        val_data_list.append(data[data['separate_num'] == i])\n",
    "    for i in range(split_num):\n",
    "        val_data_list[i] = train_data_list[i].drop(['separate_num'], axis = 1)\n",
    "        val_data_list = data_randomization(train_data_list[i])\n",
    "    return val_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_x(data_list:list) -> list:\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_y(data_list:list) -> list:\n",
    "    list_size = len(data_list)\n",
    "    data_list = []\n",
    "    for i in range(list_size):\n",
    "        data_list.append(data_list[i][[\"W\",\"D\",\"L\"]])\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def translate_pandas_to_numpy_x(data_list:list) -> list:\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].values.astype('float32')\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def translate_pandas_to_numpy_y(data_list:list) -> list:\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].values\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_processing_for_stratified_sampling(data:pandas.core.frame.DataFrame, split_num:int) -> pandas.core.frame.DataFrame:\n",
    "    win_data = data_extraction(data, 'W')\n",
    "    draw_data = data_extraction(data, 'D')\n",
    "    lose_data = data_extraction(data, 'L')\n",
    "    win_separated_data_list = divided_data_making_for_stratified_sampling(win_data, split_num, 'W')\n",
    "    draw_separated_data_list = divided_data_making_for_stratified_sampling(draw_data, split_num, 'D')\n",
    "    lose_separated_data_list = divided_data_making_for_stratified_sampling(lose_data, split_num, 'L')\n",
    "    separated_data_list = data_list_wdl_merge(win_separated_data_list\n",
    "                                          , draw_separated_data_list\n",
    "                                          , lose_separated_data_list)\n",
    "    separated_data_list = assign_group_numbers_to_data(separated_data_list)\n",
    "    integrated_data = data_list_put_together(separated_data_list)\n",
    "    return integrated_data\n",
    "\n",
    "\n",
    "def making_x_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    x_train_data_list = making_dataset_list_x(train_data)\n",
    "    return translate_pandas_to_numpy_x(x_train_data_list)\n",
    "\n",
    "\n",
    "def making_x_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    x_val_data_list = making_dataset_list_x(val_data)\n",
    "    return translate_pandas_to_numpy_x(x_val_data_list)\n",
    "\n",
    "\n",
    "def making_y_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    y_train_data_list = making_dataset_list_y(train_data)\n",
    "    return translate_pandas_to_numpy_y(y_train_data_list)\n",
    "\n",
    "\n",
    "def making_y_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    y_val_data_list = making_dataset_list_y(val_data)\n",
    "    return translate_pandas_to_numpy_y(y_val_data_list)\n",
    "\n",
    "\n",
    "def RPS(y_true, y_pred):\n",
    "    output = 0.\n",
    "    data_num = len(y_true)\n",
    "    for i in range(data_num):\n",
    "        times = len(y_true[i]) - 1 \n",
    "        cumulative_sum = 0.\n",
    "        score = 0.\n",
    "        for time in range(times):\n",
    "            cumulative_sum += y_true[i,time] - y_pred[i,time]\n",
    "            score += cumulative_sum ** 2\n",
    "        score /= times\n",
    "        output += score\n",
    "    \n",
    "    output /= data_num\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'separate_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'separate_num'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cb7092a708ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# dataset making for stratified sampling and k-fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_processing_for_stratified_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaking_x_train_data_list_for_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaking_x_test_data_list_for_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-c4c78b2d13d2>\u001b[0m in \u001b[0;36mdata_processing_for_stratified_sampling\u001b[0;34m(data, split_num)\u001b[0m\n\u001b[1;32m    136\u001b[0m                                           \u001b[0;34m,\u001b[0m \u001b[0mdraw_separated_data_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                                           , lose_separated_data_list)\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mseparated_data_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_group_numbers_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparated_data_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mintegrated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list_put_together\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparated_data_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mintegrated_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-c4c78b2d13d2>\u001b[0m in \u001b[0;36massign_group_numbers_to_data\u001b[0;34m(data_list)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparate_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'separate_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'separate_num'"
     ]
    }
   ],
   "source": [
    "# data input \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "\n",
    "# dataset making for stratified sampling and k-fold\n",
    "data = data_processing_for_stratified_sampling(data,split_num)\n",
    "x_train = making_x_train_data_list_for_kfold(data,split_num)\n",
    "x_test = making_x_test_data_list_for_kfold(data,split_num)\n",
    "y_train = making_y_train_data_list_for_kfold(data,split_num)\n",
    "y_test = making_y_test_data_list_for_kfold(data,split_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_num = 10\n",
    "\n",
    "\n",
    "# data input \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "\n",
    "\n",
    "# Data extraction,Win\n",
    "def data_exrtaction_win(data :):\n",
    "win_data = data[data.label == \"W\"]\n",
    "win_data = win_data.assign(W=1,D=0,L=0) \n",
    "win_data = win_data.drop(\"label\",axis=1)\n",
    "win_data = win_data.sample(n=len(win_data))# random sort\n",
    "\n",
    "# Data extraction,Draw\n",
    "draw_data = data[data.label == \"D\"]\n",
    "draw_data = draw_data.assign(W=0,D=1,L=0)\n",
    "draw_data = draw_data.drop(\"label\",axis=1)\n",
    "draw_data = draw_data.sample(n=len(draw_data))# random sort\n",
    "\n",
    "# Data extraction,Lose\n",
    "lose_data = data[data.label == \"L\"]\n",
    "lose_data = lose_data.assign(W=0,D=0,L=1) \n",
    "lose_data = lose_data.drop(\"label\",axis=1)\n",
    "lose_data = lose_data.sample(n=len(lose_data))# random sort\n",
    "\n",
    "\n",
    "\n",
    "# Data separate and making dataset\n",
    "win_data_separate = []\n",
    "draw_data_separate = []\n",
    "lose_data_separate = []\n",
    "all_data_separate = []\n",
    "wdl_separate = []\n",
    "for i in range(split_num):\n",
    "    win_data_separate.append(win_data[i::split_num])\n",
    "    draw_data_separate.append(draw_data[i::split_num])\n",
    "    lose_data_separate.append(lose_data[i::split_num])\n",
    "    all_data_separate.append(all_data[i::split_num])\n",
    "    # merge for stratified sampling\n",
    "    wdl_separate.append(pd.concat([win_data_separate[i],draw_data_separate[i],lose_data_separate[i]]))\n",
    "    # assign a number to make final input data\n",
    "    wdl_separate[i] = wdl_separate[i].assign(separate_num=i)\n",
    "    all_data_separate[i] = all_data_separate[i].assign(separate_num=i)\n",
    "\n",
    "# integrate everything once\n",
    "wdl_separate_merge = wdl_separate[0]\n",
    "all_data_separate_merge = all_data_separate[0]\n",
    "for i in range(1,split_num):\n",
    "    wdl_separate_merge = wdl_separate_merge.append(wdl_separate[i])\n",
    "    all_data_separate_merge = all_data_separate_merge.append(all_data_separate[i])\n",
    "\n",
    "    \n",
    "    \n",
    "# make final input data\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "xAll_train = []\n",
    "yAll_train = []\n",
    "xAll_val = []\n",
    "yAll_val = []\n",
    "for i in range(split_num):\n",
    "    x_train.append(wdl_separate_merge[wdl_separate_merge['separate_num'] != i])\n",
    "    x_val.append(wdl_separate_merge[wdl_separate_merge['separate_num'] == i])\n",
    "    xAll_train.append(all_data_separate_merge[all_data_separate_merge['separate_num'] != i])\n",
    "    xAll_val.append(all_data_separate_merge[all_data_separate_merge['separate_num'] == i])\n",
    "for i in range(split_num):\n",
    "    # delete separate_num\n",
    "    x_train[i] = x_train[i].drop(['separate_num'],axis=1)\n",
    "    x_val[i] = x_val[i].drop(['separate_num'],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop(['separate_num'],axis=1)\n",
    "    xAll_val[i] = xAll_val[i].drop(['separate_num'],axis=1)\n",
    "    # random sort\n",
    "    x_train[i] = x_train[i].sample(n=len(x_train[i]))\n",
    "    x_val[i] = x_val[i].sample(n=len(x_val[i]))\n",
    "    xAll_train[i] = xAll_train[i].sample(n=len(xAll_train[i]))\n",
    "    xAll_val[i] = xAll_val[i].sample(n=len(xAll_val[i]))\n",
    "    \n",
    "    # separate x and y\n",
    "    y_train.append(x_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    y_val.append(x_val[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_train.append(xAll_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_val.append(xAll_val[i][[\"W\",\"D\",\"L\"]])\n",
    "    x_train[i] = x_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    x_val[i] = x_val[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_val[i] = xAll_val[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    \n",
    "    #translate pandas to numpy\n",
    "    x_train[i] = x_train[i].values.astype('float32') \n",
    "    x_val[i] = x_val[i].values.astype('float32') \n",
    "    y_train[i] = y_train[i].values\n",
    "    y_val[i] = y_val[i].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 397, loss (train): 0.2134, loss (valid): 0.2079\n",
      "epoch: 1, iteration: 794, loss (train): 0.2100, loss (valid): 0.2075\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2096, loss (valid): 0.2075\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2095, loss (valid): 0.2075\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2093, loss (valid): 0.2077\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2093, loss (valid): 0.2075\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2093, loss (valid): 0.2075\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2092, loss (valid): 0.2074\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2092, loss (valid): 0.2074\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2092, loss (valid): 0.2074\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2090, loss (valid): 0.2072\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2091, loss (valid): 0.2071\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2090, loss (valid): 0.2072\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2091, loss (valid): 0.2073\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2091, loss (valid): 0.2074\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2090, loss (valid): 0.2072\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2090, loss (valid): 0.2073\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2090, loss (valid): 0.2074\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2090, loss (valid): 0.2073\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2089, loss (valid): 0.2074\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2089, loss (valid): 0.2072\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2089, loss (valid): 0.2074\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2089, loss (valid): 0.2072\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2089, loss (valid): 0.2075\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2088, loss (valid): 0.2070\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2088, loss (valid): 0.2072\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2088, loss (valid): 0.2072\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2089, loss (valid): 0.2073\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2090, loss (valid): 0.2071\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2089, loss (valid): 0.2072\n",
      "epoch: 0, iteration: 397, loss (train): 0.2119, loss (valid): 0.2084\n",
      "epoch: 1, iteration: 794, loss (train): 0.2101, loss (valid): 0.2083\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2099, loss (valid): 0.2076\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2097, loss (valid): 0.2076\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2096, loss (valid): 0.2078\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2096, loss (valid): 0.2076\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2095, loss (valid): 0.2075\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2095, loss (valid): 0.2076\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2095, loss (valid): 0.2073\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2094, loss (valid): 0.2079\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2094, loss (valid): 0.2079\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2094, loss (valid): 0.2076\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2092, loss (valid): 0.2082\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2094, loss (valid): 0.2080\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2093, loss (valid): 0.2074\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2092, loss (valid): 0.2071\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2092, loss (valid): 0.2070\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2092, loss (valid): 0.2069\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2092, loss (valid): 0.2067\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2091, loss (valid): 0.2069\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2091, loss (valid): 0.2066\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2091, loss (valid): 0.2068\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2091, loss (valid): 0.2066\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2090, loss (valid): 0.2067\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2091, loss (valid): 0.2067\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2091, loss (valid): 0.2065\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2092, loss (valid): 0.2065\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2090, loss (valid): 0.2065\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2091, loss (valid): 0.2065\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2092, loss (valid): 0.2064\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2089, loss (valid): 0.2065\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2091, loss (valid): 0.2064\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2090, loss (valid): 0.2065\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2089, loss (valid): 0.2065\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2090, loss (valid): 0.2063\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2090, loss (valid): 0.2064\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2090, loss (valid): 0.2067\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2090, loss (valid): 0.2066\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2089, loss (valid): 0.2067\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2091, loss (valid): 0.2071\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2091, loss (valid): 0.2071\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2090, loss (valid): 0.2068\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2090, loss (valid): 0.2071\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2089, loss (valid): 0.2070\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2090, loss (valid): 0.2066\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2089, loss (valid): 0.2066\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2088, loss (valid): 0.2070\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2089, loss (valid): 0.2068\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2089, loss (valid): 0.2070\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2089, loss (valid): 0.2069\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2088, loss (valid): 0.2072\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2088, loss (valid): 0.2065\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2087, loss (valid): 0.2067\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2089, loss (valid): 0.2063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55, iteration: 22232, loss (train): 0.2089, loss (valid): 0.2070\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2089, loss (valid): 0.2070\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2089, loss (valid): 0.2069\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2087, loss (valid): 0.2069\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2090, loss (valid): 0.2066\n",
      "epoch: 0, iteration: 397, loss (train): 0.2123, loss (valid): 0.2091\n",
      "epoch: 1, iteration: 794, loss (train): 0.2104, loss (valid): 0.2085\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2101, loss (valid): 0.2084\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2099, loss (valid): 0.2084\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2098, loss (valid): 0.2083\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2097, loss (valid): 0.2085\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2096, loss (valid): 0.2084\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2095, loss (valid): 0.2084\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2094, loss (valid): 0.2083\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2094, loss (valid): 0.2082\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2093, loss (valid): 0.2082\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2093, loss (valid): 0.2082\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2093, loss (valid): 0.2082\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2092, loss (valid): 0.2080\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2091, loss (valid): 0.2079\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2091, loss (valid): 0.2079\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2091, loss (valid): 0.2080\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2090, loss (valid): 0.2082\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2092, loss (valid): 0.2082\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2091, loss (valid): 0.2083\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2090, loss (valid): 0.2083\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2092, loss (valid): 0.2084\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2092, loss (valid): 0.2085\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2090, loss (valid): 0.2083\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2091, loss (valid): 0.2084\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2091, loss (valid): 0.2079\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2091, loss (valid): 0.2082\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2089, loss (valid): 0.2085\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2089, loss (valid): 0.2088\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2090, loss (valid): 0.2083\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2090, loss (valid): 0.2082\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2090, loss (valid): 0.2086\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2089, loss (valid): 0.2082\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2089, loss (valid): 0.2085\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2089, loss (valid): 0.2085\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2088, loss (valid): 0.2085\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2090, loss (valid): 0.2083\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2089, loss (valid): 0.2085\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2089, loss (valid): 0.2082\n",
      "epoch: 0, iteration: 397, loss (train): 0.2111, loss (valid): 0.2105\n",
      "epoch: 1, iteration: 794, loss (train): 0.2095, loss (valid): 0.2108\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2093, loss (valid): 0.2106\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2092, loss (valid): 0.2106\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2091, loss (valid): 0.2106\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2090, loss (valid): 0.2104\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2090, loss (valid): 0.2103\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2090, loss (valid): 0.2103\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2090, loss (valid): 0.2103\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2090, loss (valid): 0.2104\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2089, loss (valid): 0.2105\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2089, loss (valid): 0.2106\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2089, loss (valid): 0.2104\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2089, loss (valid): 0.2104\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2089, loss (valid): 0.2100\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2088, loss (valid): 0.2103\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2089, loss (valid): 0.2102\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2089, loss (valid): 0.2104\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2088, loss (valid): 0.2104\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2088, loss (valid): 0.2105\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2087, loss (valid): 0.2100\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2086, loss (valid): 0.2109\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2086, loss (valid): 0.2103\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2086, loss (valid): 0.2101\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2086, loss (valid): 0.2102\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2085, loss (valid): 0.2106\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2085, loss (valid): 0.2102\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2084, loss (valid): 0.2102\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2085, loss (valid): 0.2104\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2085, loss (valid): 0.2102\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2085, loss (valid): 0.2103\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2085, loss (valid): 0.2102\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2085, loss (valid): 0.2104\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2085, loss (valid): 0.2104\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2084, loss (valid): 0.2101\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2085, loss (valid): 0.2104\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2085, loss (valid): 0.2102\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2086, loss (valid): 0.2106\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2084, loss (valid): 0.2102\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2084, loss (valid): 0.2102\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2084, loss (valid): 0.2106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50, iteration: 20247, loss (train): 0.2085, loss (valid): 0.2104\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2084, loss (valid): 0.2102\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2084, loss (valid): 0.2103\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2083, loss (valid): 0.2102\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2085, loss (valid): 0.2106\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2084, loss (valid): 0.2103\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2084, loss (valid): 0.2105\n",
      "epoch: 0, iteration: 397, loss (train): 0.2134, loss (valid): 0.2103\n",
      "epoch: 1, iteration: 794, loss (train): 0.2103, loss (valid): 0.2097\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2097, loss (valid): 0.2098\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2094, loss (valid): 0.2097\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2093, loss (valid): 0.2098\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2091, loss (valid): 0.2098\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2091, loss (valid): 0.2096\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2090, loss (valid): 0.2094\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2089, loss (valid): 0.2095\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2089, loss (valid): 0.2095\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2090, loss (valid): 0.2093\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2090, loss (valid): 0.2093\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2090, loss (valid): 0.2093\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2090, loss (valid): 0.2094\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2089, loss (valid): 0.2092\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2088, loss (valid): 0.2091\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2088, loss (valid): 0.2091\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2088, loss (valid): 0.2092\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2088, loss (valid): 0.2092\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2087, loss (valid): 0.2091\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2087, loss (valid): 0.2089\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2087, loss (valid): 0.2091\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2087, loss (valid): 0.2091\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2087, loss (valid): 0.2093\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2088, loss (valid): 0.2092\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2087, loss (valid): 0.2093\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2087, loss (valid): 0.2093\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2087, loss (valid): 0.2093\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2086, loss (valid): 0.2093\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2086, loss (valid): 0.2093\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2086, loss (valid): 0.2093\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2086, loss (valid): 0.2093\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2086, loss (valid): 0.2095\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2086, loss (valid): 0.2095\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2086, loss (valid): 0.2097\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2087, loss (valid): 0.2096\n",
      "epoch: 0, iteration: 397, loss (train): 0.2125, loss (valid): 0.2140\n",
      "epoch: 1, iteration: 794, loss (train): 0.2105, loss (valid): 0.2133\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2103, loss (valid): 0.2131\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2100, loss (valid): 0.2131\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2099, loss (valid): 0.2131\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2099, loss (valid): 0.2130\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2098, loss (valid): 0.2130\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2097, loss (valid): 0.2130\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2096, loss (valid): 0.2130\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2096, loss (valid): 0.2131\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2095, loss (valid): 0.2130\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2096, loss (valid): 0.2129\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2095, loss (valid): 0.2131\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2095, loss (valid): 0.2136\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2094, loss (valid): 0.2132\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2095, loss (valid): 0.2133\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2094, loss (valid): 0.2132\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2094, loss (valid): 0.2129\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2094, loss (valid): 0.2130\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2094, loss (valid): 0.2132\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2093, loss (valid): 0.2125\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2092, loss (valid): 0.2129\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2091, loss (valid): 0.2131\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2094, loss (valid): 0.2129\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2091, loss (valid): 0.2129\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2092, loss (valid): 0.2134\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2091, loss (valid): 0.2136\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2092, loss (valid): 0.2132\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2091, loss (valid): 0.2133\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2092, loss (valid): 0.2131\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2090, loss (valid): 0.2129\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2091, loss (valid): 0.2131\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2092, loss (valid): 0.2128\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2091, loss (valid): 0.2128\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2091, loss (valid): 0.2131\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2092, loss (valid): 0.2130\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2092, loss (valid): 0.2133\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2093, loss (valid): 0.2133\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2093, loss (valid): 0.2134\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2093, loss (valid): 0.2133\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2093, loss (valid): 0.2134\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2092, loss (valid): 0.2131\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2089, loss (valid): 0.2129\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2090, loss (valid): 0.2129\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2091, loss (valid): 0.2129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45, iteration: 18262, loss (train): 0.2090, loss (valid): 0.2130\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2092, loss (valid): 0.2130\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2092, loss (valid): 0.2129\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2091, loss (valid): 0.2128\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2092, loss (valid): 0.2138\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2092, loss (valid): 0.2132\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2090, loss (valid): 0.2127\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2092, loss (valid): 0.2139\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2092, loss (valid): 0.2131\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2091, loss (valid): 0.2131\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2091, loss (valid): 0.2127\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2090, loss (valid): 0.2130\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2091, loss (valid): 0.2126\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2090, loss (valid): 0.2127\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2090, loss (valid): 0.2131\n",
      "epoch: 0, iteration: 397, loss (train): 0.2116, loss (valid): 0.2111\n",
      "epoch: 1, iteration: 794, loss (train): 0.2096, loss (valid): 0.2107\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2094, loss (valid): 0.2104\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2091, loss (valid): 0.2104\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2090, loss (valid): 0.2105\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2089, loss (valid): 0.2104\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2088, loss (valid): 0.2103\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2087, loss (valid): 0.2103\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2088, loss (valid): 0.2105\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2087, loss (valid): 0.2104\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2088, loss (valid): 0.2106\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2086, loss (valid): 0.2105\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2086, loss (valid): 0.2106\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2086, loss (valid): 0.2106\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2086, loss (valid): 0.2108\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2085, loss (valid): 0.2106\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2084, loss (valid): 0.2107\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2085, loss (valid): 0.2107\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2084, loss (valid): 0.2104\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2084, loss (valid): 0.2107\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2084, loss (valid): 0.2108\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2084, loss (valid): 0.2105\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2084, loss (valid): 0.2105\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2084, loss (valid): 0.2107\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2084, loss (valid): 0.2107\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2084, loss (valid): 0.2107\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2084, loss (valid): 0.2106\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2084, loss (valid): 0.2105\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2084, loss (valid): 0.2108\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2085, loss (valid): 0.2107\n",
      "epoch: 37, iteration: 15086, loss (train): 0.2084, loss (valid): 0.2109\n",
      "epoch: 38, iteration: 15483, loss (train): 0.2083, loss (valid): 0.2106\n",
      "epoch: 39, iteration: 15880, loss (train): 0.2084, loss (valid): 0.2109\n",
      "epoch: 40, iteration: 16277, loss (train): 0.2084, loss (valid): 0.2108\n",
      "epoch: 41, iteration: 16674, loss (train): 0.2083, loss (valid): 0.2107\n",
      "epoch: 42, iteration: 17071, loss (train): 0.2083, loss (valid): 0.2109\n",
      "epoch: 43, iteration: 17468, loss (train): 0.2083, loss (valid): 0.2109\n",
      "epoch: 44, iteration: 17865, loss (train): 0.2083, loss (valid): 0.2108\n",
      "epoch: 45, iteration: 18262, loss (train): 0.2084, loss (valid): 0.2109\n",
      "epoch: 46, iteration: 18659, loss (train): 0.2083, loss (valid): 0.2109\n",
      "epoch: 47, iteration: 19056, loss (train): 0.2083, loss (valid): 0.2110\n",
      "epoch: 48, iteration: 19453, loss (train): 0.2083, loss (valid): 0.2108\n",
      "epoch: 49, iteration: 19850, loss (train): 0.2083, loss (valid): 0.2110\n",
      "epoch: 50, iteration: 20247, loss (train): 0.2082, loss (valid): 0.2109\n",
      "epoch: 51, iteration: 20644, loss (train): 0.2082, loss (valid): 0.2109\n",
      "epoch: 52, iteration: 21041, loss (train): 0.2082, loss (valid): 0.2109\n",
      "epoch: 53, iteration: 21438, loss (train): 0.2082, loss (valid): 0.2110\n",
      "epoch: 54, iteration: 21835, loss (train): 0.2082, loss (valid): 0.2110\n",
      "epoch: 55, iteration: 22232, loss (train): 0.2082, loss (valid): 0.2110\n",
      "epoch: 56, iteration: 22629, loss (train): 0.2081, loss (valid): 0.2109\n",
      "epoch: 57, iteration: 23026, loss (train): 0.2082, loss (valid): 0.2108\n",
      "epoch: 58, iteration: 23423, loss (train): 0.2082, loss (valid): 0.2108\n",
      "epoch: 59, iteration: 23820, loss (train): 0.2081, loss (valid): 0.2107\n",
      "epoch: 0, iteration: 397, loss (train): 0.2117, loss (valid): 0.2094\n",
      "epoch: 1, iteration: 794, loss (train): 0.2097, loss (valid): 0.2098\n",
      "epoch: 2, iteration: 1191, loss (train): 0.2095, loss (valid): 0.2094\n",
      "epoch: 3, iteration: 1588, loss (train): 0.2094, loss (valid): 0.2097\n",
      "epoch: 4, iteration: 1985, loss (train): 0.2094, loss (valid): 0.2091\n",
      "epoch: 5, iteration: 2382, loss (train): 0.2092, loss (valid): 0.2091\n",
      "epoch: 6, iteration: 2779, loss (train): 0.2092, loss (valid): 0.2091\n",
      "epoch: 7, iteration: 3176, loss (train): 0.2092, loss (valid): 0.2090\n",
      "epoch: 8, iteration: 3573, loss (train): 0.2092, loss (valid): 0.2091\n",
      "epoch: 9, iteration: 3970, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 10, iteration: 4367, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 11, iteration: 4764, loss (train): 0.2091, loss (valid): 0.2090\n",
      "epoch: 12, iteration: 5161, loss (train): 0.2090, loss (valid): 0.2089\n",
      "epoch: 13, iteration: 5558, loss (train): 0.2089, loss (valid): 0.2089\n",
      "epoch: 14, iteration: 5955, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 15, iteration: 6352, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 16, iteration: 6749, loss (train): 0.2089, loss (valid): 0.2085\n",
      "epoch: 17, iteration: 7146, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 18, iteration: 7543, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 19, iteration: 7940, loss (train): 0.2088, loss (valid): 0.2084\n",
      "epoch: 20, iteration: 8337, loss (train): 0.2088, loss (valid): 0.2087\n",
      "epoch: 21, iteration: 8734, loss (train): 0.2088, loss (valid): 0.2084\n",
      "epoch: 22, iteration: 9131, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 23, iteration: 9528, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 24, iteration: 9925, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 25, iteration: 10322, loss (train): 0.2088, loss (valid): 0.2085\n",
      "epoch: 26, iteration: 10719, loss (train): 0.2088, loss (valid): 0.2084\n",
      "epoch: 27, iteration: 11116, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 28, iteration: 11513, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 29, iteration: 11910, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 30, iteration: 12307, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 31, iteration: 12704, loss (train): 0.2087, loss (valid): 0.2084\n",
      "epoch: 32, iteration: 13101, loss (train): 0.2087, loss (valid): 0.2088\n",
      "epoch: 33, iteration: 13498, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 34, iteration: 13895, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 35, iteration: 14292, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 36, iteration: 14689, loss (train): 0.2087, loss (valid): 0.2087\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rate_list = [0.01]\n",
    "batchsize_list = [64]\n",
    "\n",
    "# craete model\n",
    "# net \n",
    "n_input = 8\n",
    "n_hidden = 5\n",
    "n_output = 3\n",
    "\n",
    "n_epoch = 60\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    for batchsize in batchsize_list:\n",
    "        # create 10 model for 10-fold-crossvalidation\n",
    "        optimizer = []\n",
    "        net = []\n",
    "\n",
    "        for i in range(split_num):\n",
    "            net.append(Sequential(\n",
    "                L.Linear(n_input, n_hidden), F.relu,\n",
    "                L.Linear(n_hidden, n_hidden), F.relu,\n",
    "                L.Linear(n_hidden, n_hidden), F.relu,\n",
    "                L.Linear(n_hidden, n_output), F.softmax)\n",
    "            )\n",
    "            optimizer.append(chainer.optimizers.Adam(alpha=learning_rate))\n",
    "            optimizer[i].setup(net[i])\n",
    "\n",
    "        # \n",
    "        results_train_data = []\n",
    "        results_valid_data = []\n",
    "\n",
    "\n",
    "        for data_num in range(len(x_train)):\n",
    "            # \n",
    "            results_train = {\n",
    "                'loss': [],\n",
    "                'accuracy': []\n",
    "            }\n",
    "            results_valid = {\n",
    "                'loss': [],\n",
    "                'accuracy': []\n",
    "            }\n",
    "            iteration = 0\n",
    "            for epoch in range(n_epoch):\n",
    "                # \n",
    "                loss_list = []\n",
    "                #accuracy_list = []\n",
    "\n",
    "                for i in range(0, len(x_train[data_num]), batchsize):\n",
    "                    # \n",
    "                    x_train_batch = x_train[data_num][i:i+batchsize,:]\n",
    "                    y_train_batch = y_train[data_num][i:i+batchsize,:]\n",
    "\n",
    "                    # \n",
    "                    y_train_batch_pred = net[data_num](x_train_batch)\n",
    "                    # \n",
    "                    loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "\n",
    "                    loss_list.append(loss_train_batch.array)\n",
    "\n",
    "                    # \n",
    "                    net[data_num].cleargrads()\n",
    "                    loss_train_batch.backward()\n",
    "\n",
    "                    # \n",
    "                    optimizer[data_num].update()\n",
    "\n",
    "                    # \n",
    "                    iteration += 1\n",
    "    \n",
    "                # \n",
    "                loss_train = np.mean(loss_list)\n",
    "\n",
    "                # 1\n",
    "                # \n",
    "                with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "                    y_val_pred = net[data_num](x_val[data_num])\n",
    "\n",
    "                # \n",
    "                loss_val = RPS(y_val_pred, y_val[data_num])\n",
    "\n",
    "                # \n",
    "                print('epoch: {}, iteration: {}, loss (train): {:.4f}, loss (valid): {:.4f}'.format(\n",
    "                    epoch, iteration, loss_train, loss_val.array))\n",
    "\n",
    "                # \n",
    "                results_train['loss'] .append(loss_train)\n",
    "                #results_train['accuracy'] .append(accuracy_train)\n",
    "                results_valid['loss'].append(loss_val.array)\n",
    "                #results_valid['accuracy'].append(accuracy_val.array)\n",
    "\n",
    "            results_train_data.append(results_train)\n",
    "            results_valid_data.append(results_valid)\n",
    "\n",
    "        results_train_data_all = []\n",
    "        results_valid_data_all = []\n",
    "        results_train_data_all = np.zeros(n_epoch)\n",
    "        results_valid_data_all = np.zeros(n_epoch)\n",
    "        #  (loss)\n",
    "        for i in range(split_num):   \n",
    "            results_train_data_all += results_train_data[i]['loss']\n",
    "            results_valid_data_all += results_valid_data[i]['loss']\n",
    "\n",
    "        print('learning_rate: {}, batch_size: {}'.format(learning_rate,batchsize))\n",
    "        plt.plot(results_train_data_all / split_num, label='train')  # label \n",
    "        plt.plot(results_valid_data_all / split_num, label='valid')  # label \n",
    "        plt.legend()  # \n",
    "        plt.figure()\n",
    "\n",
    "        print('train: {}'.format(results_train_data_all / split_num))\n",
    "        print('valid: {}'.format(results_valid_data_all / split_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
