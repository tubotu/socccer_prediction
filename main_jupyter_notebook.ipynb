{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tubotu/.local/lib/python3.6/site-packages/chainer/backends/cuda.py:143: UserWarning: cuDNN is not enabled.\n",
      "Please reinstall CuPy after you install cudnn\n",
      "(see https://docs-cupy.chainer.org/en/stable/install.html#install-cudnn).\n",
      "  'cuDNN is not enabled.\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core\n",
    "import chainer.optimizers\n",
    "from chainer import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# constant setting\n",
    "split_num = 10\n",
    "n_input = 8\n",
    "n_hidden = 20\n",
    "n_output = 3\n",
    "alpha = 0.0001\n",
    "n_epoch = 50\n",
    "batchsize = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_column_conversion(data:pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Change the label element from str to int.\n",
    "\n",
    "    @param data: the extracted data\n",
    "    @return: the conversed data\n",
    "    \"\"\"\n",
    "    data = data.assign(W = (data.label == 'W') + 0,D = (data.label == 'D') + 0,L = (data.label == 'L') + 0)\n",
    "    data = data.drop(\"label\",axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_randomization(data:pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomize data.\n",
    "\n",
    "    @param data: the conversed data you want to separate\n",
    "    @return: the randomized data\n",
    "    \"\"\"\n",
    "    return data.sample(n = len(data))\n",
    "\n",
    "\n",
    "def data_separate(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Separate data.\n",
    "    \n",
    "    @param data: the data you want to separate\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: separated data list\n",
    "    \"\"\"\n",
    "    data_separate = []\n",
    "    for i in range(split_num):\n",
    "        data_separate.append(data[i::split_num])\n",
    "    return data_separate\n",
    "\n",
    "\n",
    "def data_list_wdl_merge(data_list1:list, data_list2:list) -> list:\n",
    "    \"\"\"\n",
    "    Join two data lists by array number.\n",
    "    \n",
    "    @param data_list1: the data list you want to join\n",
    "    @param data_list2: the data list you want to join\n",
    "    @return: Joined data list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list1)\n",
    "    merged_data_list = []\n",
    "    for i in range(list_size):\n",
    "        merged_data_list.append(pd.concat([data_list1[i],data_list2[i]]))\n",
    "    return merged_data_list\n",
    "\n",
    "\n",
    "def assign_group_numbers_to_data(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Assign group numbers to data.\n",
    "    Group number is array numbers of data list.\n",
    "    \n",
    "    @param data_list: the data list you want to assign group numbers\n",
    "    @return data_list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].assign(separate_num=i)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_list_put_together(data_list:list) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine the data list into one DataFrame.\n",
    "    \n",
    "    @param data_list: the data list you want to assign group numbers\n",
    "    @return collected data\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    data = data_list[0]\n",
    "    for i in range(1,list_size):\n",
    "        data = data.append(data_list[i])\n",
    "    return data\n",
    "\n",
    "\n",
    "def making_dataset_list_train(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Create training data set list for cross validation using group number.\n",
    "    \n",
    "    @param data: the collected data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return training data set list\n",
    "    \"\"\"\n",
    "    train_data_list = []\n",
    "    for i in range(split_num):\n",
    "        train_data_list.append(data[data['separate_num'] != i])\n",
    "    for i in range(split_num):\n",
    "        train_data_list[i] = train_data_list[i].drop(['separate_num'], axis = 1)\n",
    "    return train_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_val(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Create validation data set list for cross validation using group number.\n",
    "    \n",
    "    @param data: the collected data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = []\n",
    "    for i in range(split_num):\n",
    "        val_data_list.append(data[data['separate_num'] == i])\n",
    "    for i in range(split_num):\n",
    "        val_data_list[i] = val_data_list[i].drop(['separate_num'], axis = 1)\n",
    "    return val_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_x(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Create data set list for only explanatory variables.\n",
    "    \n",
    "    @param data_list: data set list\n",
    "    @return: data set list for only explanatory variables\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_y(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Create data set list for only target variables.\n",
    "    \n",
    "    @param data_list: data set list\n",
    "    @return: data set list for only target variables\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    data_list_y = []\n",
    "    for i in range(list_size):\n",
    "        data_list_y.append(data_list[i][[\"W\",\"D\",\"L\"]])\n",
    "    return data_list_y\n",
    "\n",
    "\n",
    "def translate_pandas_to_numpy(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Convert pandas to numpy and make type 'float32'.\n",
    "    \n",
    "    @param data_list:pandas data set list\n",
    "    @return: numpy data set list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].values.astype('float32')\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_processing_for_stratified_sampling(data:pandas.core.frame.DataFrame, split_num:int) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the data for stratified sampling.\n",
    "    \n",
    "    @param data: inputted data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: the data for stratified sampling\n",
    "    \"\"\"\n",
    "    win_data = data[data.label == 'W'] # Extract only the data of label of win\n",
    "    draw_data = data[data.label == 'D'] # Extract only the data of label of draw\n",
    "    lose_data = data[data.label == 'L'] # Extract only the data of label of lose\n",
    "    data_list = [win_data, draw_data, lose_data]\n",
    "    for i,wdl_data in enumerate(data_list):\n",
    "        wdl_data = data_column_conversion(wdl_data)\n",
    "        wdl_data = data_randomization(wdl_data)\n",
    "        if i == 0:\n",
    "            separated_data_list = data_separate(wdl_data, split_num)\n",
    "        else:\n",
    "            separated_data_list = data_list_wdl_merge(separated_data_list, data_separate(wdl_data, split_num))\n",
    "    separated_data_list = assign_group_numbers_to_data(separated_data_list)\n",
    "    integrated_data = data_list_put_together(separated_data_list)\n",
    "    integrated_data = data_randomization(integrated_data)\n",
    "    return integrated_data\n",
    "\n",
    "\n",
    "def making_x_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making x train data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: x train data set list\n",
    "    \"\"\"\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    x_train_data_list = making_dataset_list_x(train_data_list)\n",
    "    return translate_pandas_to_numpy(x_train_data_list)\n",
    "\n",
    "\n",
    "def making_x_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making x validation data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: x validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    x_val_data_list = making_dataset_list_x(val_data_list)\n",
    "    return translate_pandas_to_numpy(x_val_data_list)\n",
    "\n",
    "\n",
    "def making_y_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making y train data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: y train data set list\n",
    "    \"\"\"\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    y_train_data_list = making_dataset_list_y(train_data_list)\n",
    "    return translate_pandas_to_numpy(y_train_data_list)\n",
    "\n",
    "\n",
    "def making_y_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making y validation data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: y validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    y_val_data_list = making_dataset_list_y(val_data_list)\n",
    "    return translate_pandas_to_numpy(y_val_data_list)\n",
    "\n",
    "\n",
    "def RPS(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    Calcurate loss by RPS.\n",
    "    \n",
    "    @param y_true: the answer list of target variable. ex.)(0.6,0.3,0.1)\n",
    "    @param y_pred: the predict list of target variable. ex.)(0.4,0.4,0.2)\n",
    "    @return: the value of loss\n",
    "    \"\"\"\n",
    "    output = 0.\n",
    "    data_num = len(y_true)\n",
    "    for i in range(data_num):\n",
    "        times = len(y_true[i]) - 1 \n",
    "        cumulative_sum = 0.\n",
    "        score = 0.\n",
    "        for time in range(times):\n",
    "            cumulative_sum += y_true[i,time] - y_pred[i,time]\n",
    "            score += cumulative_sum ** 2\n",
    "        score /= times\n",
    "        output += score\n",
    "    \n",
    "    output /= data_num\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_model_net(n_input,n_hidden,n_output):\n",
    "    \"\"\"\n",
    "    Create net of model.\n",
    "    \n",
    "    @param n_input: number of neurons in input layer\n",
    "    @param n_hidden: number of neurons in hidden layer\n",
    "    @param n_hidden: number of neurons in output layer\n",
    "    @return: net\n",
    "    \"\"\"\n",
    "    net = Sequential(\n",
    "        L.Linear(n_input, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_output), F.softmax)\n",
    "    return net\n",
    "\n",
    "\n",
    "def create_model_optimizer(net,alpha):\n",
    "    \"\"\"\n",
    "    Create optimizer of model.\n",
    "    \n",
    "    @param net: net of using model\n",
    "    @param alpha: learning rate\n",
    "    @return: optimizer\n",
    "    \"\"\"\n",
    "    optimizer = chainer.optimizers.Adam(alpha=alpha)\n",
    "    optimizer.setup(net)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input data \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "\n",
    "path2 = 'testrat_new.txt'\n",
    "data2 = pd.read_csv(path2,sep=' ')\n",
    "\n",
    "data = pd.concat([data,data2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset making for stratified sampling and k-fold\n",
    "s_data = data_processing_for_stratified_sampling(data,split_num)\n",
    "x_train = making_x_train_data_list_for_kfold(s_data,split_num)\n",
    "x_val = making_x_val_data_list_for_kfold(s_data,split_num)\n",
    "y_train = making_y_train_data_list_for_kfold(s_data,split_num)\n",
    "y_val = making_y_val_data_list_for_kfold(s_data,split_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = []\n",
    "net = []\n",
    "\n",
    "for i in range(10):\n",
    "    net.append(create_model_net(n_input,n_hidden,n_output))\n",
    "    optimizer.append(create_model_optimizer(net[i],alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 1762, loss (train): 0.2207, loss (valid): 0.2137\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2107, loss (valid): 0.2118\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2097, loss (valid): 0.2113\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2093, loss (valid): 0.2110\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2091, loss (valid): 0.2108\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2089, loss (valid): 0.2107\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2088, loss (valid): 0.2106\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2086, loss (valid): 0.2106\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2085, loss (valid): 0.2105\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2084, loss (valid): 0.2105\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2083, loss (valid): 0.2104\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2083, loss (valid): 0.2104\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2082, loss (valid): 0.2104\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2081, loss (valid): 0.2104\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2081, loss (valid): 0.2104\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2080, loss (valid): 0.2105\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2080, loss (valid): 0.2105\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2080, loss (valid): 0.2105\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2079, loss (valid): 0.2105\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2079, loss (valid): 0.2105\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2078, loss (valid): 0.2105\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2078, loss (valid): 0.2105\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2078, loss (valid): 0.2105\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2077, loss (valid): 0.2105\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2077, loss (valid): 0.2106\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2077, loss (valid): 0.2106\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2077, loss (valid): 0.2106\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2076, loss (valid): 0.2106\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2076, loss (valid): 0.2106\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2076, loss (valid): 0.2106\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2076, loss (valid): 0.2106\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2075, loss (valid): 0.2107\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2075, loss (valid): 0.2107\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2075, loss (valid): 0.2107\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2075, loss (valid): 0.2107\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2074, loss (valid): 0.2107\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2074, loss (valid): 0.2107\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2074, loss (valid): 0.2107\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2074, loss (valid): 0.2108\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2074, loss (valid): 0.2108\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2073, loss (valid): 0.2108\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2073, loss (valid): 0.2108\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2073, loss (valid): 0.2108\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2073, loss (valid): 0.2108\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2073, loss (valid): 0.2108\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2072, loss (valid): 0.2109\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2072, loss (valid): 0.2109\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2072, loss (valid): 0.2109\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2072, loss (valid): 0.2109\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2162, loss (valid): 0.2123\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2112, loss (valid): 0.2101\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2101, loss (valid): 0.2093\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2097, loss (valid): 0.2088\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2094, loss (valid): 0.2085\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2092, loss (valid): 0.2083\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2089, loss (valid): 0.2080\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2088, loss (valid): 0.2079\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2087, loss (valid): 0.2078\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2086, loss (valid): 0.2078\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2085, loss (valid): 0.2077\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2085, loss (valid): 0.2077\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2084, loss (valid): 0.2076\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2083, loss (valid): 0.2076\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2082, loss (valid): 0.2075\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2082, loss (valid): 0.2075\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2081, loss (valid): 0.2074\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2081, loss (valid): 0.2074\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2080, loss (valid): 0.2074\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2080, loss (valid): 0.2074\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2080, loss (valid): 0.2074\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2079, loss (valid): 0.2074\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2079, loss (valid): 0.2074\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2079, loss (valid): 0.2074\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2078, loss (valid): 0.2074\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2078, loss (valid): 0.2074\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2078, loss (valid): 0.2073\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2077, loss (valid): 0.2073\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2077, loss (valid): 0.2073\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2077, loss (valid): 0.2073\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2076, loss (valid): 0.2073\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2076, loss (valid): 0.2073\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2076, loss (valid): 0.2073\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2075, loss (valid): 0.2073\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2075, loss (valid): 0.2073\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2075, loss (valid): 0.2073\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2075, loss (valid): 0.2073\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2074, loss (valid): 0.2073\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2074, loss (valid): 0.2073\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2074, loss (valid): 0.2073\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2073, loss (valid): 0.2073\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2073, loss (valid): 0.2073\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2073, loss (valid): 0.2073\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2073, loss (valid): 0.2073\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2072, loss (valid): 0.2073\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2072, loss (valid): 0.2073\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2072, loss (valid): 0.2073\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2208, loss (valid): 0.2130\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2115, loss (valid): 0.2107\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2103, loss (valid): 0.2101\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2097, loss (valid): 0.2098\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2094, loss (valid): 0.2096\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2092, loss (valid): 0.2095\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2091, loss (valid): 0.2095\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2090, loss (valid): 0.2095\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2089, loss (valid): 0.2094\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2088, loss (valid): 0.2094\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2085, loss (valid): 0.2094\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2085, loss (valid): 0.2094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, iteration: 28192, loss (train): 0.2084, loss (valid): 0.2094\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2084, loss (valid): 0.2094\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2083, loss (valid): 0.2093\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2083, loss (valid): 0.2093\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2082, loss (valid): 0.2093\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2081, loss (valid): 0.2093\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2081, loss (valid): 0.2093\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2081, loss (valid): 0.2093\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2080, loss (valid): 0.2093\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2080, loss (valid): 0.2093\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2079, loss (valid): 0.2093\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2079, loss (valid): 0.2093\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2078, loss (valid): 0.2093\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2078, loss (valid): 0.2093\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2078, loss (valid): 0.2093\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2077, loss (valid): 0.2093\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2077, loss (valid): 0.2093\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2077, loss (valid): 0.2093\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2076, loss (valid): 0.2093\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2076, loss (valid): 0.2092\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2076, loss (valid): 0.2092\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2076, loss (valid): 0.2092\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2075, loss (valid): 0.2092\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2075, loss (valid): 0.2092\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2075, loss (valid): 0.2092\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2074, loss (valid): 0.2092\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2074, loss (valid): 0.2092\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2074, loss (valid): 0.2092\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2074, loss (valid): 0.2092\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2073, loss (valid): 0.2092\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2073, loss (valid): 0.2092\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2073, loss (valid): 0.2092\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2073, loss (valid): 0.2092\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2072, loss (valid): 0.2092\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2072, loss (valid): 0.2092\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2213, loss (valid): 0.2138\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2103, loss (valid): 0.2123\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2093, loss (valid): 0.2121\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2090, loss (valid): 0.2120\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2088, loss (valid): 0.2119\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2086, loss (valid): 0.2118\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2085, loss (valid): 0.2117\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2084, loss (valid): 0.2117\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2084, loss (valid): 0.2117\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2083, loss (valid): 0.2116\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2082, loss (valid): 0.2116\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2082, loss (valid): 0.2116\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2081, loss (valid): 0.2116\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2081, loss (valid): 0.2116\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2080, loss (valid): 0.2116\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2080, loss (valid): 0.2115\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2079, loss (valid): 0.2115\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2079, loss (valid): 0.2115\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2079, loss (valid): 0.2115\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2078, loss (valid): 0.2115\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2078, loss (valid): 0.2115\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2078, loss (valid): 0.2115\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2077, loss (valid): 0.2115\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2077, loss (valid): 0.2115\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2077, loss (valid): 0.2115\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2076, loss (valid): 0.2115\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2076, loss (valid): 0.2115\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2076, loss (valid): 0.2115\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2076, loss (valid): 0.2115\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2075, loss (valid): 0.2115\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2075, loss (valid): 0.2115\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2075, loss (valid): 0.2115\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2075, loss (valid): 0.2115\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2075, loss (valid): 0.2116\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2074, loss (valid): 0.2116\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2074, loss (valid): 0.2116\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2074, loss (valid): 0.2116\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2074, loss (valid): 0.2116\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2074, loss (valid): 0.2116\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2073, loss (valid): 0.2116\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2073, loss (valid): 0.2116\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2073, loss (valid): 0.2116\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2073, loss (valid): 0.2116\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2073, loss (valid): 0.2116\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2072, loss (valid): 0.2116\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2072, loss (valid): 0.2116\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2072, loss (valid): 0.2116\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2072, loss (valid): 0.2116\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2072, loss (valid): 0.2116\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2071, loss (valid): 0.2116\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2200, loss (valid): 0.2115\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2115, loss (valid): 0.2100\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2103, loss (valid): 0.2096\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2097, loss (valid): 0.2094\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2093, loss (valid): 0.2093\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2090, loss (valid): 0.2093\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2089, loss (valid): 0.2093\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2083, loss (valid): 0.2090\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2083, loss (valid): 0.2090\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2082, loss (valid): 0.2090\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2082, loss (valid): 0.2090\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2081, loss (valid): 0.2090\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2081, loss (valid): 0.2090\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2080, loss (valid): 0.2089\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2080, loss (valid): 0.2089\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2080, loss (valid): 0.2089\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2079, loss (valid): 0.2089\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2079, loss (valid): 0.2089\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2078, loss (valid): 0.2089\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2078, loss (valid): 0.2089\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2078, loss (valid): 0.2089\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2077, loss (valid): 0.2089\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2077, loss (valid): 0.2089\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2077, loss (valid): 0.2089\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2077, loss (valid): 0.2089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, iteration: 54622, loss (train): 0.2076, loss (valid): 0.2089\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2076, loss (valid): 0.2089\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2076, loss (valid): 0.2089\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2076, loss (valid): 0.2089\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2075, loss (valid): 0.2089\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2075, loss (valid): 0.2089\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2075, loss (valid): 0.2089\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2075, loss (valid): 0.2089\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2074, loss (valid): 0.2089\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2074, loss (valid): 0.2089\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2074, loss (valid): 0.2089\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2074, loss (valid): 0.2089\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2074, loss (valid): 0.2089\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2073, loss (valid): 0.2089\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2073, loss (valid): 0.2090\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2073, loss (valid): 0.2090\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2073, loss (valid): 0.2090\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2073, loss (valid): 0.2090\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2072, loss (valid): 0.2090\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2072, loss (valid): 0.2090\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2176, loss (valid): 0.2137\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2119, loss (valid): 0.2118\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2105, loss (valid): 0.2109\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2097, loss (valid): 0.2105\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2093, loss (valid): 0.2102\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2090, loss (valid): 0.2100\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2088, loss (valid): 0.2099\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2087, loss (valid): 0.2098\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2086, loss (valid): 0.2098\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2085, loss (valid): 0.2097\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2084, loss (valid): 0.2097\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2083, loss (valid): 0.2097\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2083, loss (valid): 0.2096\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2082, loss (valid): 0.2096\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2082, loss (valid): 0.2096\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2081, loss (valid): 0.2095\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2081, loss (valid): 0.2095\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2080, loss (valid): 0.2095\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2080, loss (valid): 0.2095\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2080, loss (valid): 0.2095\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2079, loss (valid): 0.2095\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2079, loss (valid): 0.2095\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2079, loss (valid): 0.2095\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2078, loss (valid): 0.2095\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2078, loss (valid): 0.2095\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2078, loss (valid): 0.2095\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2078, loss (valid): 0.2095\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2077, loss (valid): 0.2095\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2077, loss (valid): 0.2095\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2077, loss (valid): 0.2095\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2077, loss (valid): 0.2094\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2076, loss (valid): 0.2094\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2076, loss (valid): 0.2094\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2076, loss (valid): 0.2094\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2076, loss (valid): 0.2094\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2076, loss (valid): 0.2094\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2075, loss (valid): 0.2094\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2075, loss (valid): 0.2094\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2075, loss (valid): 0.2094\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2075, loss (valid): 0.2094\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2075, loss (valid): 0.2094\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2074, loss (valid): 0.2094\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2073, loss (valid): 0.2094\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2073, loss (valid): 0.2093\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2162, loss (valid): 0.2100\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2107, loss (valid): 0.2086\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2100, loss (valid): 0.2081\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2097, loss (valid): 0.2078\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2095, loss (valid): 0.2076\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2093, loss (valid): 0.2074\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2092, loss (valid): 0.2073\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2091, loss (valid): 0.2072\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2090, loss (valid): 0.2072\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2089, loss (valid): 0.2071\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2088, loss (valid): 0.2070\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2088, loss (valid): 0.2070\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2087, loss (valid): 0.2070\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2087, loss (valid): 0.2069\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2086, loss (valid): 0.2069\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2086, loss (valid): 0.2068\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2086, loss (valid): 0.2068\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2085, loss (valid): 0.2068\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2085, loss (valid): 0.2068\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2085, loss (valid): 0.2068\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2084, loss (valid): 0.2067\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2084, loss (valid): 0.2067\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2084, loss (valid): 0.2067\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2083, loss (valid): 0.2067\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2083, loss (valid): 0.2067\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2083, loss (valid): 0.2066\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2083, loss (valid): 0.2066\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2082, loss (valid): 0.2066\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2082, loss (valid): 0.2066\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2082, loss (valid): 0.2066\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2082, loss (valid): 0.2066\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2081, loss (valid): 0.2065\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2081, loss (valid): 0.2065\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2081, loss (valid): 0.2065\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2081, loss (valid): 0.2065\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2081, loss (valid): 0.2065\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2080, loss (valid): 0.2065\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2080, loss (valid): 0.2065\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2080, loss (valid): 0.2065\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2080, loss (valid): 0.2065\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2080, loss (valid): 0.2065\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2079, loss (valid): 0.2064\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2079, loss (valid): 0.2064\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2079, loss (valid): 0.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45, iteration: 81052, loss (train): 0.2079, loss (valid): 0.2064\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2079, loss (valid): 0.2064\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2078, loss (valid): 0.2064\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2078, loss (valid): 0.2064\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2078, loss (valid): 0.2064\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2219, loss (valid): 0.2114\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2110, loss (valid): 0.2085\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2098, loss (valid): 0.2079\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2094, loss (valid): 0.2077\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2088, loss (valid): 0.2075\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2087, loss (valid): 0.2075\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2086, loss (valid): 0.2075\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2085, loss (valid): 0.2075\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2085, loss (valid): 0.2075\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2084, loss (valid): 0.2075\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2084, loss (valid): 0.2075\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2082, loss (valid): 0.2076\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2082, loss (valid): 0.2076\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2082, loss (valid): 0.2076\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2081, loss (valid): 0.2076\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2081, loss (valid): 0.2076\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2081, loss (valid): 0.2076\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2080, loss (valid): 0.2076\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2080, loss (valid): 0.2076\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2080, loss (valid): 0.2077\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2079, loss (valid): 0.2077\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2079, loss (valid): 0.2077\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2079, loss (valid): 0.2077\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2078, loss (valid): 0.2077\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2078, loss (valid): 0.2077\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2078, loss (valid): 0.2077\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2078, loss (valid): 0.2077\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2077, loss (valid): 0.2077\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2077, loss (valid): 0.2077\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2077, loss (valid): 0.2077\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2076, loss (valid): 0.2077\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2076, loss (valid): 0.2077\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2076, loss (valid): 0.2077\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2076, loss (valid): 0.2077\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2075, loss (valid): 0.2077\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2075, loss (valid): 0.2078\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2075, loss (valid): 0.2078\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2075, loss (valid): 0.2078\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2075, loss (valid): 0.2078\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2074, loss (valid): 0.2078\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2074, loss (valid): 0.2078\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2074, loss (valid): 0.2078\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2074, loss (valid): 0.2078\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2073, loss (valid): 0.2078\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2073, loss (valid): 0.2078\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2073, loss (valid): 0.2078\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2312, loss (valid): 0.2172\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2127, loss (valid): 0.2124\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2105, loss (valid): 0.2113\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2099, loss (valid): 0.2109\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2096, loss (valid): 0.2105\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2094, loss (valid): 0.2102\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2092, loss (valid): 0.2100\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2091, loss (valid): 0.2098\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2090, loss (valid): 0.2097\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2089, loss (valid): 0.2096\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2088, loss (valid): 0.2096\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2088, loss (valid): 0.2095\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2087, loss (valid): 0.2094\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2086, loss (valid): 0.2094\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2086, loss (valid): 0.2093\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2085, loss (valid): 0.2093\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2085, loss (valid): 0.2093\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2085, loss (valid): 0.2092\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2084, loss (valid): 0.2092\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2084, loss (valid): 0.2092\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2084, loss (valid): 0.2092\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2083, loss (valid): 0.2092\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2083, loss (valid): 0.2091\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2083, loss (valid): 0.2091\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2082, loss (valid): 0.2091\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2082, loss (valid): 0.2091\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2082, loss (valid): 0.2091\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2082, loss (valid): 0.2091\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2081, loss (valid): 0.2091\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2081, loss (valid): 0.2091\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2081, loss (valid): 0.2091\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2081, loss (valid): 0.2091\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2081, loss (valid): 0.2091\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2080, loss (valid): 0.2091\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2080, loss (valid): 0.2091\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2080, loss (valid): 0.2091\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2080, loss (valid): 0.2091\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2080, loss (valid): 0.2091\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2079, loss (valid): 0.2091\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2079, loss (valid): 0.2091\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2079, loss (valid): 0.2091\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2079, loss (valid): 0.2091\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2079, loss (valid): 0.2091\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2078, loss (valid): 0.2091\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2078, loss (valid): 0.2091\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2078, loss (valid): 0.2091\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2078, loss (valid): 0.2091\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2078, loss (valid): 0.2091\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2077, loss (valid): 0.2091\n",
      "epoch: 0, iteration: 1762, loss (train): 0.2195, loss (valid): 0.2115\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2119, loss (valid): 0.2094\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2109, loss (valid): 0.2086\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2103, loss (valid): 0.2082\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2100, loss (valid): 0.2080\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2097, loss (valid): 0.2078\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2095, loss (valid): 0.2077\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2093, loss (valid): 0.2077\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2091, loss (valid): 0.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, iteration: 19382, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2089, loss (valid): 0.2075\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2088, loss (valid): 0.2075\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2087, loss (valid): 0.2075\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2087, loss (valid): 0.2075\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2086, loss (valid): 0.2075\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2085, loss (valid): 0.2075\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2085, loss (valid): 0.2075\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2084, loss (valid): 0.2075\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2084, loss (valid): 0.2075\n",
      "epoch: 20, iteration: 37002, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 21, iteration: 38764, loss (train): 0.2083, loss (valid): 0.2075\n",
      "epoch: 22, iteration: 40526, loss (train): 0.2082, loss (valid): 0.2075\n",
      "epoch: 23, iteration: 42288, loss (train): 0.2082, loss (valid): 0.2075\n",
      "epoch: 24, iteration: 44050, loss (train): 0.2082, loss (valid): 0.2075\n",
      "epoch: 25, iteration: 45812, loss (train): 0.2081, loss (valid): 0.2075\n",
      "epoch: 26, iteration: 47574, loss (train): 0.2081, loss (valid): 0.2075\n",
      "epoch: 27, iteration: 49336, loss (train): 0.2080, loss (valid): 0.2075\n",
      "epoch: 28, iteration: 51098, loss (train): 0.2080, loss (valid): 0.2075\n",
      "epoch: 29, iteration: 52860, loss (train): 0.2080, loss (valid): 0.2075\n",
      "epoch: 30, iteration: 54622, loss (train): 0.2080, loss (valid): 0.2075\n",
      "epoch: 31, iteration: 56384, loss (train): 0.2079, loss (valid): 0.2075\n",
      "epoch: 32, iteration: 58146, loss (train): 0.2079, loss (valid): 0.2075\n",
      "epoch: 33, iteration: 59908, loss (train): 0.2079, loss (valid): 0.2075\n",
      "epoch: 34, iteration: 61670, loss (train): 0.2078, loss (valid): 0.2075\n",
      "epoch: 35, iteration: 63432, loss (train): 0.2078, loss (valid): 0.2075\n",
      "epoch: 36, iteration: 65194, loss (train): 0.2078, loss (valid): 0.2075\n",
      "epoch: 37, iteration: 66956, loss (train): 0.2078, loss (valid): 0.2075\n",
      "epoch: 38, iteration: 68718, loss (train): 0.2078, loss (valid): 0.2075\n",
      "epoch: 39, iteration: 70480, loss (train): 0.2077, loss (valid): 0.2075\n",
      "epoch: 40, iteration: 72242, loss (train): 0.2077, loss (valid): 0.2075\n",
      "epoch: 41, iteration: 74004, loss (train): 0.2077, loss (valid): 0.2075\n",
      "epoch: 42, iteration: 75766, loss (train): 0.2077, loss (valid): 0.2075\n",
      "epoch: 43, iteration: 77528, loss (train): 0.2076, loss (valid): 0.2075\n",
      "epoch: 44, iteration: 79290, loss (train): 0.2076, loss (valid): 0.2075\n",
      "epoch: 45, iteration: 81052, loss (train): 0.2076, loss (valid): 0.2074\n",
      "epoch: 46, iteration: 82814, loss (train): 0.2076, loss (valid): 0.2074\n",
      "epoch: 47, iteration: 84576, loss (train): 0.2076, loss (valid): 0.2074\n",
      "epoch: 48, iteration: 86338, loss (train): 0.2075, loss (valid): 0.2074\n",
      "epoch: 49, iteration: 88100, loss (train): 0.2075, loss (valid): 0.2074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f02c4baaef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell does k-fold cross validation.\n",
    "Basically, only one of this cell or the cell below is used.\n",
    "\"\"\"\n",
    "\n",
    "# dataset making for stratified sampling and k-fold\n",
    "s_data = data_processing_for_stratified_sampling(data,split_num)\n",
    "x_train = making_x_train_data_list_for_kfold(s_data,split_num)\n",
    "x_val = making_x_val_data_list_for_kfold(s_data,split_num)\n",
    "y_train = making_y_train_data_list_for_kfold(s_data,split_num)\n",
    "y_val = making_y_val_data_list_for_kfold(s_data,split_num)\n",
    "\n",
    "\n",
    "# create optimizer\n",
    "optimizer = []\n",
    "net = []\n",
    "for i in range(10):\n",
    "    net.append(create_model_net(n_input,n_hidden,n_output))\n",
    "    optimizer.append(create_model_optimizer(net[i],alpha))\n",
    "\n",
    "\n",
    "# for log storage\n",
    "results_train_data = []\n",
    "results_valid_data = []\n",
    "\n",
    "for data_num in range(len(x_train)):\n",
    "    # for log storage\n",
    "    results_train = []\n",
    "    results_valid = []\n",
    "    # counting\n",
    "    iteration = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        # output of objective function for each batch and storage of classification accuracy\n",
    "        loss_list = []\n",
    "\n",
    "        for i in range(0, len(x_train[data_num]), batchsize):\n",
    "            # batch preparation\n",
    "            x_train_batch = x_train[data_num][i:i+batchsize,:]\n",
    "            y_train_batch = y_train[data_num][i:i+batchsize,:]\n",
    "\n",
    "            # output predicted value\n",
    "            y_train_batch_pred = net[data_num](x_train_batch)\n",
    "            # apply objective function to calculate classification accuracy\n",
    "            loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "            loss_list.append(loss_train_batch.array)\n",
    "\n",
    "            # slope reset and slope calculation\n",
    "            net[data_num].cleargrads()\n",
    "            loss_train_batch.backward()\n",
    "\n",
    "            # parameter update\n",
    "            optimizer[data_num].update()\n",
    "\n",
    "            # count up\n",
    "            iteration += 1\n",
    "\n",
    "        # output objective function for training data, and aggregate classification accuracy\n",
    "        loss_train = np.mean(loss_list)\n",
    "\n",
    "        # evaluate with validation data every time the epoch is over\n",
    "        # output predicted values in validation data\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "            y_val_pred = net[data_num](x_val[data_num])\n",
    "\n",
    "        # apply objective function to calculate classification accuracy\n",
    "        loss_val = RPS(y_val_pred, y_val[data_num])\n",
    "\n",
    "        # display the result\n",
    "        print('epoch: {}, iteration: {}, loss (train): {:.4f}, loss (valid): {:.4f}'.format(\n",
    "            epoch, iteration, loss_train, loss_val.array))\n",
    "\n",
    "        # log storage\n",
    "        results_train.append(loss_train)\n",
    "        results_valid.append(loss_val.array)\n",
    "\n",
    "    # log storage\n",
    "    results_train_data.append(results_train)\n",
    "    results_valid_data.append(results_valid)\n",
    "    \n",
    "\n",
    "# calcurate average\n",
    "results_train_data_all = []\n",
    "results_valid_data_all = []\n",
    "results_train_data_all = np.zeros(epoch+1)\n",
    "results_valid_data_all = np.zeros(epoch+1)\n",
    "for i in range(split_num):   \n",
    "    results_train_data_all += results_train_data[i]\n",
    "    results_valid_data_all += results_valid_data[i]\n",
    "results_train_data_ave = results_train_data_all / 10\n",
    "results_valid_data_ave = results_valid_data_all / 10\n",
    "\n",
    "# output of objective function\n",
    "plt.plot(results_train_data_ave, label='train')  # set legend with label\n",
    "plt.plot(results_valid_data_ave, label='valid')  # set legend with label\n",
    "plt.legend()  # display legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell outputs the predicted value for test data.\n",
    "A model is built from scratch to train with all data.\n",
    "\"\"\"\n",
    "\n",
    "# create optimizer\n",
    "production_net = create_model_net(n_input,n_hidden,n_output)\n",
    "production_optimizer = create_model_optimizer(production_net,alpha)\n",
    "\n",
    "# make dataset\n",
    "c_data = data_column_conversion(data)\n",
    "c_data = data_randomization(c_data)\n",
    "x_train = c_data.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "x_train = x_train.values.astype('float32')\n",
    "y_train = c_data[[\"W\",\"D\",\"L\"]]\n",
    "y_train = y_train.values.astype('float32')\n",
    "\n",
    "\n",
    "# log storage\n",
    "results_train = []\n",
    "iteration = 0\n",
    "for epoch in range(n_epoch):\n",
    "    loss_list = []\n",
    "    for i in range(0, len(x_train), batchsize):\n",
    "        # batch preparation\n",
    "        x_train_batch = x_train[i:i+batchsize,:]\n",
    "        y_train_batch = y_train[i:i+batchsize,:]\n",
    "\n",
    "        # output predicted value\n",
    "        y_train_batch_pred = production_net(x_train_batch)\n",
    "        \n",
    "        # apply objective function to calculate classification accuracy\n",
    "        loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "        loss_list.append(loss_train_batch.array)\n",
    "\n",
    "        # slope reset and slope calculation\n",
    "        production_net.cleargrads()\n",
    "        loss_train_batch.backward()\n",
    "\n",
    "        # parameter update\n",
    "        production_optimizer.update()\n",
    "\n",
    "        # count up\n",
    "        iteration += 1\n",
    "        \n",
    "    # output objective function for training data, and aggregate classification accuracy\n",
    "    loss_train = np.mean(loss_list)\n",
    "    \n",
    "    # display the result\n",
    "    print('epoch: {}, iteration: {}, loss (train): {:.4f}'.format(epoch, iteration, loss_train))\n",
    "    \n",
    "    # log storage\n",
    "    results_train.append(loss_train)\n",
    "    \n",
    "\n",
    "# input data\n",
    "path3 = 'predrat_20190708.txt'\n",
    "data3 = pd.read_csv(path3,sep=',')\n",
    "\n",
    "# making test dataset\n",
    "test_data = data3.drop([\"num\",\"label\"],axis=1)\n",
    "test_data = test_data.values.astype('float32')\n",
    "\n",
    "# output predicted value\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    test_pred = production_net(test_data)\n",
    "\n",
    "# unit dataset and predicted value\n",
    "output_data = np.concatenate([test_data, test_pred.data], 1)\n",
    "    \n",
    "# convert to pandas\n",
    "columns = ['HHATT','HHDEF','HAATT','HADEF','AHATT','AHDEF','AAATT','AADEF','W','D','L']\n",
    "output = pd.DataFrame(data=output_data, columns=columns, dtype='float64')\n",
    "\n",
    "# output file\n",
    "output.to_csv(\"output.txt\", sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
