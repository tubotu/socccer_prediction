{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is soccer prediction with DNN.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core\n",
    "import chainer.optimizers\n",
    "from chainer import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# constant setting\n",
    "split_num = 10\n",
    "n_input = 8\n",
    "n_hidden = 20\n",
    "n_output = 3\n",
    "alpha = 0.0001\n",
    "n_epoch = 50\n",
    "batchsize = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_column_conversion(data:pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Change the label element from str to int.\n",
    "\n",
    "    @param data: the extracted data\n",
    "    @return: the conversed data\n",
    "    \"\"\"\n",
    "    data = data.assign(W = (data.label == 'W') + 0,D = (data.label == 'D') + 0,L = (data.label == 'L') + 0)\n",
    "    data = data.drop(\"label\",axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_randomization(data:pandas.core.frame.DataFrame) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomize data.\n",
    "\n",
    "    @param data: the conversed data you want to separate\n",
    "    @return: the randomized data\n",
    "    \"\"\"\n",
    "    return data.sample(n = len(data))\n",
    "\n",
    "\n",
    "def data_separate(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Separate data.\n",
    "    \n",
    "    @param data: the data you want to separate\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: separated data list\n",
    "    \"\"\"\n",
    "    data_separate = []\n",
    "    for i in range(split_num):\n",
    "        data_separate.append(data[i::split_num])\n",
    "    return data_separate\n",
    "\n",
    "\n",
    "def data_list_wdl_merge(data_list1:list, data_list2:list) -> list:\n",
    "    \"\"\"\n",
    "    Join two data lists by array number.\n",
    "    \n",
    "    @param data_list1: the data list you want to join\n",
    "    @param data_list2: the data list you want to join\n",
    "    @return: Joined data list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list1)\n",
    "    merged_data_list = []\n",
    "    for i in range(list_size):\n",
    "        merged_data_list.append(pd.concat([data_list1[i],data_list2[i]]))\n",
    "    return merged_data_list\n",
    "\n",
    "\n",
    "def assign_group_numbers_to_data(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Assign group numbers to data.\n",
    "    Group number is array numbers of data list.\n",
    "    \n",
    "    @param data_list: the data list you want to assign group numbers\n",
    "    @return data_list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].assign(separate_num=i)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_list_put_together(data_list:list) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine the data list into one DataFrame.\n",
    "    \n",
    "    @param data_list: the data list you want to assign group numbers\n",
    "    @return collected data\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    data = data_list[0]\n",
    "    for i in range(1,list_size):\n",
    "        data = data.append(data_list[i])\n",
    "    return data\n",
    "\n",
    "\n",
    "def making_dataset_list_train(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Create training data set list for cross validation using group number.\n",
    "    \n",
    "    @param data: the collected data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return training data set list\n",
    "    \"\"\"\n",
    "    train_data_list = []\n",
    "    for i in range(split_num):\n",
    "        train_data_list.append(data[data['separate_num'] != i])\n",
    "    for i in range(split_num):\n",
    "        train_data_list[i] = train_data_list[i].drop(['separate_num'], axis = 1)\n",
    "    return train_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_val(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Create validation data set list for cross validation using group number.\n",
    "    \n",
    "    @param data: the collected data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = []\n",
    "    for i in range(split_num):\n",
    "        val_data_list.append(data[data['separate_num'] == i])\n",
    "    for i in range(split_num):\n",
    "        val_data_list[i] = val_data_list[i].drop(['separate_num'], axis = 1)\n",
    "    return val_data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_x(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Create data set list for only explanatory variables.\n",
    "    \n",
    "    @param data_list: data set list\n",
    "    @return: data set list for only explanatory variables\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def making_dataset_list_y(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Create data set list for only target variables.\n",
    "    \n",
    "    @param data_list: data set list\n",
    "    @return: data set list for only target variables\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    data_list_y = []\n",
    "    for i in range(list_size):\n",
    "        data_list_y.append(data_list[i][[\"W\",\"D\",\"L\"]])\n",
    "    return data_list_y\n",
    "\n",
    "\n",
    "def translate_pandas_to_numpy(data_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Convert pandas to numpy and make type 'float32'.\n",
    "    \n",
    "    @param data_list:pandas data set list\n",
    "    @return: numpy data set list\n",
    "    \"\"\"\n",
    "    list_size = len(data_list)\n",
    "    for i in range(list_size):\n",
    "        data_list[i] = data_list[i].values.astype('float32')\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def data_processing_for_stratified_sampling(data:pandas.core.frame.DataFrame, split_num:int) -> pandas.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Process the data for stratified sampling.\n",
    "    \n",
    "    @param data: inputted data\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: the data for stratified sampling\n",
    "    \"\"\"\n",
    "    win_data = data[data.label == 'W'] # Extract only the data of label of win\n",
    "    draw_data = data[data.label == 'D'] # Extract only the data of label of draw\n",
    "    lose_data = data[data.label == 'L'] # Extract only the data of label of lose\n",
    "    data_list = [win_data, draw_data, lose_data]\n",
    "    for i,wdl_data in enumerate(data_list):\n",
    "        wdl_data = data_column_conversion(wdl_data)\n",
    "        wdl_data = data_randomization(wdl_data)\n",
    "        if i == 0:\n",
    "            separated_data_list = data_separate(wdl_data, split_num)\n",
    "        else:\n",
    "            separated_data_list = data_list_wdl_merge(separated_data_list, data_separate(wdl_data, split_num))\n",
    "    separated_data_list = assign_group_numbers_to_data(separated_data_list)\n",
    "    integrated_data = data_list_put_together(separated_data_list)\n",
    "    integrated_data = data_randomization(integrated_data)\n",
    "    return integrated_data\n",
    "\n",
    "\n",
    "def making_x_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making x train data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: x train data set list\n",
    "    \"\"\"\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    x_train_data_list = making_dataset_list_x(train_data_list)\n",
    "    return translate_pandas_to_numpy(x_train_data_list)\n",
    "\n",
    "\n",
    "def making_x_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making x validation data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: x validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    x_val_data_list = making_dataset_list_x(val_data_list)\n",
    "    return translate_pandas_to_numpy(x_val_data_list)\n",
    "\n",
    "\n",
    "def making_y_train_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making y train data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: y train data set list\n",
    "    \"\"\"\n",
    "    train_data_list = making_dataset_list_train(data, split_num)\n",
    "    y_train_data_list = making_dataset_list_y(train_data_list)\n",
    "    return translate_pandas_to_numpy(y_train_data_list)\n",
    "\n",
    "\n",
    "def making_y_val_data_list_for_kfold(data:pandas.core.frame.DataFrame, split_num:int) -> list:\n",
    "    \"\"\"\n",
    "    Making y validation data list for k-fold cross validation.\n",
    "    \n",
    "    @param data: the data for stratified sampling\n",
    "    @param split_num: the division number for k-fold\n",
    "    @return: y validation data set list\n",
    "    \"\"\"\n",
    "    val_data_list = making_dataset_list_val(data, split_num)\n",
    "    y_val_data_list = making_dataset_list_y(val_data_list)\n",
    "    return translate_pandas_to_numpy(y_val_data_list)\n",
    "\n",
    "\n",
    "def RPS(y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    Calcurate loss by RPS.\n",
    "    \n",
    "    @param y_true: the answer list of target variable. ex.)(0.6,0.3,0.1)\n",
    "    @param y_pred: the predict list of target variable. ex.)(0.4,0.4,0.2)\n",
    "    @return: the value of loss\n",
    "    \"\"\"\n",
    "    output = 0.\n",
    "    data_num = len(y_true)\n",
    "    for i in range(data_num):\n",
    "        times = len(y_true[i]) - 1 \n",
    "        cumulative_sum = 0.\n",
    "        score = 0.\n",
    "        for time in range(times):\n",
    "            cumulative_sum += y_true[i,time] - y_pred[i,time]\n",
    "            score += cumulative_sum ** 2\n",
    "        score /= times\n",
    "        output += score\n",
    "    \n",
    "    output /= data_num\n",
    "    return output\n",
    "\n",
    "\n",
    "def create_model_net(n_input,n_hidden,n_output):\n",
    "    \"\"\"\n",
    "    Create net of model.\n",
    "    \n",
    "    @param n_input: number of neurons in input layer\n",
    "    @param n_hidden: number of neurons in hidden layer\n",
    "    @param n_hidden: number of neurons in output layer\n",
    "    @return: net\n",
    "    \"\"\"\n",
    "    net = Sequential(\n",
    "        L.Linear(n_input, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_output), F.softmax)\n",
    "    return net\n",
    "\n",
    "\n",
    "def create_model_optimizer(net,alpha):\n",
    "    \"\"\"\n",
    "    Create optimizer of model.\n",
    "    \n",
    "    @param net: net of using model\n",
    "    @param alpha: learning rate\n",
    "    @return: optimizer\n",
    "    \"\"\"\n",
    "    optimizer = chainer.optimizers.Adam(alpha=alpha)\n",
    "    optimizer.setup(net)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input data \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "\n",
    "path2 = 'testrat_new.txt'\n",
    "data2 = pd.read_csv(path2,sep=' ')\n",
    "\n",
    "data = pd.concat([data,data2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 1762, loss (train): 0.2170, loss (valid): 0.2119\n",
      "epoch: 1, iteration: 3524, loss (train): 0.2111, loss (valid): 0.2105\n",
      "epoch: 2, iteration: 5286, loss (train): 0.2102, loss (valid): 0.2100\n",
      "epoch: 3, iteration: 7048, loss (train): 0.2098, loss (valid): 0.2096\n",
      "epoch: 4, iteration: 8810, loss (train): 0.2095, loss (valid): 0.2094\n",
      "epoch: 5, iteration: 10572, loss (train): 0.2093, loss (valid): 0.2092\n",
      "epoch: 6, iteration: 12334, loss (train): 0.2091, loss (valid): 0.2091\n",
      "epoch: 7, iteration: 14096, loss (train): 0.2090, loss (valid): 0.2090\n",
      "epoch: 8, iteration: 15858, loss (train): 0.2089, loss (valid): 0.2089\n",
      "epoch: 9, iteration: 17620, loss (train): 0.2088, loss (valid): 0.2089\n",
      "epoch: 10, iteration: 19382, loss (train): 0.2087, loss (valid): 0.2088\n",
      "epoch: 11, iteration: 21144, loss (train): 0.2086, loss (valid): 0.2087\n",
      "epoch: 12, iteration: 22906, loss (train): 0.2085, loss (valid): 0.2087\n",
      "epoch: 13, iteration: 24668, loss (train): 0.2084, loss (valid): 0.2087\n",
      "epoch: 14, iteration: 26430, loss (train): 0.2084, loss (valid): 0.2086\n",
      "epoch: 15, iteration: 28192, loss (train): 0.2083, loss (valid): 0.2086\n",
      "epoch: 16, iteration: 29954, loss (train): 0.2083, loss (valid): 0.2086\n",
      "epoch: 17, iteration: 31716, loss (train): 0.2082, loss (valid): 0.2086\n",
      "epoch: 18, iteration: 33478, loss (train): 0.2082, loss (valid): 0.2086\n",
      "epoch: 19, iteration: 35240, loss (train): 0.2081, loss (valid): 0.2086\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell does k-fold cross validation.\n",
    "Basically, only one of this cell or the cell below is used.\n",
    "\"\"\"\n",
    "\n",
    "# dataset making for stratified sampling and k-fold\n",
    "s_data = data_processing_for_stratified_sampling(data,split_num)\n",
    "x_train = making_x_train_data_list_for_kfold(s_data,split_num)\n",
    "x_val = making_x_val_data_list_for_kfold(s_data,split_num)\n",
    "y_train = making_y_train_data_list_for_kfold(s_data,split_num)\n",
    "y_val = making_y_val_data_list_for_kfold(s_data,split_num)\n",
    "\n",
    "\n",
    "# create optimizer\n",
    "optimizer = []\n",
    "net = []\n",
    "for i in range(split_num):\n",
    "    net.append(create_model_net(n_input,n_hidden,n_output))\n",
    "    optimizer.append(create_model_optimizer(net[i],alpha))\n",
    "\n",
    "\n",
    "# for log storage\n",
    "results_train_data = []\n",
    "results_valid_data = []\n",
    "\n",
    "for data_num in range(len(x_train)):\n",
    "    # for log storage\n",
    "    results_train = []\n",
    "    results_valid = []\n",
    "    # counting\n",
    "    iteration = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        # output of objective function for each batch and storage of classification accuracy\n",
    "        loss_list = []\n",
    "\n",
    "        for i in range(0, len(x_train[data_num]), batchsize):\n",
    "            # batch preparation\n",
    "            x_train_batch = x_train[data_num][i:i+batchsize,:]\n",
    "            y_train_batch = y_train[data_num][i:i+batchsize,:]\n",
    "\n",
    "            # output predicted value\n",
    "            y_train_batch_pred = net[data_num](x_train_batch)\n",
    "            # apply objective function to calculate classification accuracy\n",
    "            loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "            loss_list.append(loss_train_batch.array)\n",
    "\n",
    "            # slope reset and slope calculation\n",
    "            net[data_num].cleargrads()\n",
    "            loss_train_batch.backward()\n",
    "\n",
    "            # parameter update\n",
    "            optimizer[data_num].update()\n",
    "\n",
    "            # count up\n",
    "            iteration += 1\n",
    "\n",
    "        # output objective function for training data, and aggregate classification accuracy\n",
    "        loss_train = np.mean(loss_list)\n",
    "\n",
    "        # evaluate with validation data every time the epoch is over\n",
    "        # output predicted values in validation data\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "            y_val_pred = net[data_num](x_val[data_num])\n",
    "\n",
    "        # apply objective function to calculate classification accuracy\n",
    "        loss_val = RPS(y_val_pred, y_val[data_num])\n",
    "\n",
    "        # display the result\n",
    "        print('epoch: {}, iteration: {}, loss (train): {:.4f}, loss (valid): {:.4f}'.format(\n",
    "            epoch, iteration, loss_train, loss_val.array))\n",
    "\n",
    "        # log storage\n",
    "        results_train.append(loss_train)\n",
    "        results_valid.append(loss_val.array)\n",
    "\n",
    "    # log storage\n",
    "    results_train_data.append(results_train)\n",
    "    results_valid_data.append(results_valid)\n",
    "    \n",
    "\n",
    "# calcurate average\n",
    "results_train_data_all = []\n",
    "results_valid_data_all = []\n",
    "results_train_data_all = np.zeros(epoch+1)\n",
    "results_valid_data_all = np.zeros(epoch+1)\n",
    "for i in range(split_num):   \n",
    "    results_train_data_all += results_train_data[i]\n",
    "    results_valid_data_all += results_valid_data[i]\n",
    "results_train_data_ave = results_train_data_all / split_num\n",
    "results_valid_data_ave = results_valid_data_all / split_num\n",
    "\n",
    "# output of objective function\n",
    "plt.plot(results_train_data_ave, label='train')  # set legend with label\n",
    "plt.plot(results_valid_data_ave, label='valid')  # set legend with label\n",
    "plt.legend()  # display legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell outputs the predicted value for test data.\n",
    "A model is built from scratch to train with all data.\n",
    "\"\"\"\n",
    "\n",
    "# create optimizer\n",
    "production_net = create_model_net(n_input,n_hidden,n_output)\n",
    "production_optimizer = create_model_optimizer(production_net,alpha)\n",
    "\n",
    "# make dataset\n",
    "c_data = data_column_conversion(data)\n",
    "c_data = data_randomization(c_data)\n",
    "x_train = c_data.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "x_train = x_train.values.astype('float32')\n",
    "y_train = c_data[[\"W\",\"D\",\"L\"]]\n",
    "y_train = y_train.values.astype('float32')\n",
    "\n",
    "\n",
    "# log storage\n",
    "results_train = []\n",
    "iteration = 0\n",
    "for epoch in range(n_epoch):\n",
    "    loss_list = []\n",
    "    for i in range(0, len(x_train), batchsize):\n",
    "        # batch preparation\n",
    "        x_train_batch = x_train[i:i+batchsize,:]\n",
    "        y_train_batch = y_train[i:i+batchsize,:]\n",
    "\n",
    "        # output predicted value\n",
    "        y_train_batch_pred = production_net(x_train_batch)\n",
    "        \n",
    "        # apply objective function to calculate classification accuracy\n",
    "        loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "        loss_list.append(loss_train_batch.array)\n",
    "\n",
    "        # slope reset and slope calculation\n",
    "        production_net.cleargrads()\n",
    "        loss_train_batch.backward()\n",
    "\n",
    "        # parameter update\n",
    "        production_optimizer.update()\n",
    "\n",
    "        # count up\n",
    "        iteration += 1\n",
    "        \n",
    "    # output objective function for training data, and aggregate classification accuracy\n",
    "    loss_train = np.mean(loss_list)\n",
    "    \n",
    "    # display the result\n",
    "    print('epoch: {}, iteration: {}, loss (train): {:.4f}'.format(epoch, iteration, loss_train))\n",
    "    \n",
    "    # log storage\n",
    "    results_train.append(loss_train)\n",
    "    \n",
    "\n",
    "# input data\n",
    "path3 = 'predrat_20190708.txt'\n",
    "data3 = pd.read_csv(path3,sep=',')\n",
    "\n",
    "# making test dataset\n",
    "test_data = data3.drop([\"num\",\"label\"],axis=1)\n",
    "test_data = test_data.values.astype('float32')\n",
    "\n",
    "# output predicted value\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    test_pred = production_net(test_data)\n",
    "\n",
    "# unit dataset and predicted value\n",
    "output_data = np.concatenate([test_data, test_pred.data], 1)\n",
    "    \n",
    "# convert to pandas\n",
    "columns = ['HHATT','HHDEF','HAATT','HADEF','AHATT','AHDEF','AAATT','AADEF','W','D','L']\n",
    "output = pd.DataFrame(data=output_data, columns=columns, dtype='float64')\n",
    "\n",
    "# output file\n",
    "output.to_csv(\"output.txt\", sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
