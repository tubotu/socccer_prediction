{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label     HHATT     HHDEF     HAATT     HADEF     AHATT     AHDEF  \\\n",
      "13680     D -0.013286  0.055444 -0.032403  0.112291 -0.013286  0.163329   \n",
      "2729      L  1.006109  0.230130  0.488624  0.489483  0.985480  0.083888   \n",
      "26413     D -0.034506  0.105019  0.012418  0.435008 -0.061290  0.057348   \n",
      "8975      D  4.744033 -1.088280  1.720375 -1.439166  0.698823  0.749711   \n",
      "13324     W  0.016564 -0.053097  0.040713  0.194267  0.026022  0.175712   \n",
      "\n",
      "          AAATT     AADEF  \n",
      "13680 -0.032403 -0.015245  \n",
      "2729   0.432996 -0.060702  \n",
      "26413  0.395053  0.766563  \n",
      "8975   0.396043  1.203952  \n",
      "13324  0.040616  0.915641  \n",
      "          HHATT     HHDEF     HAATT     HADEF     AHATT     AHDEF     AAATT  \\\n",
      "9221  -0.053163 -0.170218  0.061345  0.130607 -0.244653  1.491166  0.168156   \n",
      "9929   1.873492  0.014206  0.407465 -0.709762 -1.779173  0.074977 -0.209977   \n",
      "29197  0.347706 -0.101115 -0.434594 -0.109751 -0.054333  0.319899 -0.028997   \n",
      "290    1.100914  0.016453  0.482952 -0.289089  1.289542  0.312502  0.264482   \n",
      "21699  0.151229  0.287541  0.004691  0.329477  0.644080  0.282609  0.759856   \n",
      "\n",
      "          AADEF  \n",
      "9221   0.918362  \n",
      "9929   1.332114  \n",
      "29197 -0.309951  \n",
      "290    0.842287  \n",
      "21699  0.119834  \n",
      "       W  D  L\n",
      "9221   1  0  0\n",
      "9929   1  0  0\n",
      "29197  1  0  0\n",
      "290    1  0  0\n",
      "21699  1  0  0\n",
      "          HHATT     HHDEF     HAATT     HADEF     AHATT     AHDEF     AAATT  \\\n",
      "16971  0.842020 -0.024472  0.153570  0.026408  0.710440  0.186435  0.049522   \n",
      "1743   0.050651  0.169715  0.005112  0.048300  0.091920  0.056689  0.002030   \n",
      "7329   0.033093  0.099345  0.071490 -0.019652 -0.037720 -0.055247 -0.082435   \n",
      "1070   0.863684  0.038689  0.440543 -0.473346 -0.044623 -0.052836  0.052891   \n",
      "26042 -0.260827  0.243837 -0.293322  0.171410  0.435306  0.277550  0.368455   \n",
      "\n",
      "          AADEF  \n",
      "16971  0.147515  \n",
      "1743  -0.052234  \n",
      "7329   0.130653  \n",
      "1070  -0.038111  \n",
      "26042  0.039806  \n",
      "       W  D  L\n",
      "6314   0  0  1\n",
      "10027  1  0  0\n",
      "11370  1  0  0\n",
      "689    1  0  0\n",
      "11891  1  0  0\n",
      "25366\n",
      "2820\n",
      "25367\n",
      "2819\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# data input \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "print(data.head())\n",
    "\n",
    "# Data extraction,Win\n",
    "xWin = data[data.label == \"W\"]\n",
    "xWin = xWin.assign(W=1,D=0,L=0) \n",
    "xWin = xWin.drop(\"label\",axis=1)\n",
    "xWin = xWin.sample(n=len(xWin))#random sort\n",
    "yWin = xWin[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Data extraction,Draw\n",
    "xDraw = data[data.label == \"D\"]\n",
    "xDraw = xDraw.assign(W=0,D=1,L=0) \n",
    "xDraw = xDraw.drop(\"label\",axis=1)\n",
    "xDraw = xDraw.sample(n=len(xDraw))#random sort\n",
    "yDraw = xDraw[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Data extraction,Lose\n",
    "xLose = data[data.label == \"L\"]\n",
    "xLose = xLose.assign(W=0,D=0,L=1) \n",
    "xLose = xLose.drop(\"label\",axis=1)\n",
    "xLose = xLose.sample(n=len(xLose))#random sort\n",
    "yLose = xLose[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Group data together,use for normal k-fold\n",
    "xAll = pd.concat([xWin, xDraw, xLose])\n",
    "xAll = xAll.sample(n=len(xAll))#random sort\n",
    "yAll = xAll[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Data drop\n",
    "xWin = xWin.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "xDraw = xDraw.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "xLose = xLose.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "xAll = xAll.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "\n",
    "# Data separate and making dataset\n",
    "xWin_separate = []\n",
    "xDraw_separate = []\n",
    "xLose_separate = []\n",
    "xAll_separate = []\n",
    "yWin_separate = []\n",
    "yDraw_separate = []\n",
    "yLose_separate = []\n",
    "yAll_separate = []\n",
    "x_separate = []\n",
    "y_separate = []\n",
    "for i in range(10):\n",
    "    xWin_separate.append(xWin[i::10])\n",
    "    xDraw_separate.append(xDraw[i::10])\n",
    "    xLose_separate.append(xLose[i::10])\n",
    "    xAll_separate.append(xAll[i::10])\n",
    "    yWin_separate.append(yWin[i::10])\n",
    "    yDraw_separate.append(yDraw[i::10])\n",
    "    yLose_separate.append(yLose[i::10])\n",
    "    yAll_separate.append(yAll[i::10])\n",
    "    # merge for stratified sampling\n",
    "    x_separate.append(pd.concat([xWin_separate[i],xDraw_separate[i],xLose_separate[i]]))\n",
    "    y_separate.append(pd.concat([yWin_separate[i],yDraw_separate[i],yLose_separate[i]]))\n",
    "    # assign a number to make final input data\n",
    "    x_separate[i] = x_separate[i].assign(separate_num=i)\n",
    "    y_separate[i] = y_separate[i].assign(separate_num=i)\n",
    "    xAll_separate[i] = xAll_separate[i].assign(separate_num=i)\n",
    "    yAll_separate[i] = yAll_separate[i].assign(separate_num=i)\n",
    "\n",
    "# integrate everything once\n",
    "x_separate_merge = x_separate[0]\n",
    "y_separate_merge = y_separate[0]\n",
    "xAll_separate_merge = xAll_separate[0]\n",
    "yAll_separate_merge = yAll_separate[0]\n",
    "\n",
    "# \n",
    "for i in range(1,10):\n",
    "    x_separate_merge = x_separate_merge.append(x_separate[i])\n",
    "    y_separate_merge = y_separate_merge.append(y_separate[i])\n",
    "    xAll_separate_merge = xAll_separate_merge.append(xAll_separate[i])\n",
    "    yAll_separate_merge = yAll_separate_merge.append(yAll_separate[i])\n",
    "    \n",
    "\n",
    "# make final input data\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "xAll_train = []\n",
    "yAll_train = []\n",
    "xAll_test = []\n",
    "yAll_test = []\n",
    "for i in range(10):\n",
    "    x_train.append(x_separate_merge[x_separate_merge['separate_num'] != i])\n",
    "    y_train.append(y_separate_merge[y_separate_merge['separate_num'] != i])\n",
    "    x_test.append(x_separate_merge[x_separate_merge['separate_num'] == i])\n",
    "    y_test.append(y_separate_merge[y_separate_merge['separate_num'] == i])\n",
    "    xAll_train.append(xAll_separate_merge[xAll_separate_merge['separate_num'] != i])\n",
    "    yAll_train.append(yAll_separate_merge[yAll_separate_merge['separate_num'] != i])\n",
    "    xAll_test.append(xAll_separate_merge[xAll_separate_merge['separate_num'] == i])\n",
    "    yAll_test.append(yAll_separate_merge[yAll_separate_merge['separate_num'] == i])\n",
    "    # delete separate_num\n",
    "    x_train[i] = x_train[i].drop(['separate_num'],axis=1)\n",
    "    x_test[i] = x_test[i].drop(['separate_num'],axis=1)\n",
    "    y_train[i] = y_train[i].drop(['separate_num'],axis=1)\n",
    "    y_test[i] = y_test[i].drop(['separate_num'],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop(['separate_num'],axis=1)\n",
    "    xAll_test[i] = xAll_test[i].drop(['separate_num'],axis=1)\n",
    "    yAll_train[i] = yAll_train[i].drop(['separate_num'],axis=1)\n",
    "    yAll_test[i] = yAll_test[i].drop(['separate_num'],axis=1)\n",
    "\n",
    "\n",
    "print(x_train[0].head())\n",
    "print(y_train[0].head())\n",
    "print(xAll_train[0].head())\n",
    "print(yAll_test[0].head())\n",
    "\n",
    "print(len(x_train[0]))\n",
    "print(len(y_test[0]))\n",
    "print(len(xAll_train[0]))\n",
    "print(len(yAll_test[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "modelArelu = Sequential()\n",
    "modelArelu.add(Dense(10, activation='relu'))\n",
    "modelArelu.add(Dense(10, activation='relu'))\n",
    "modelArelu.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return \n",
    "\n",
    "modelArelu.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     3 ... 28183 28184 28185] TEST: [    2     8    17 ... 28166 28173 28176]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 37us/step - loss: 0.6569 - acc: 0.6059\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6515 - acc: 0.6147\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6509 - acc: 0.6170\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6506 - acc: 0.6153\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6503 - acc: 0.6142\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6499 - acc: 0.6164\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6500 - acc: 0.6158\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6495 - acc: 0.6173\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6157\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6497 - acc: 0.6158\n",
      "[[0.4044243 ]\n",
      " [0.7105591 ]\n",
      " [0.840841  ]\n",
      " ...\n",
      " [0.43311858]\n",
      " [0.44360483]\n",
      " [0.5276969 ]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   13    21    54 ... 28165 28170 28171]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6494 - acc: 0.6164\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6491 - acc: 0.6196\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6489 - acc: 0.6184\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6490 - acc: 0.6167\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6490 - acc: 0.6185\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6489 - acc: 0.6178\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6488 - acc: 0.6180\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6487 - acc: 0.6190\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6175\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6486 - acc: 0.6188\n",
      "[[0.41986367]\n",
      " [0.4078181 ]\n",
      " [0.4373505 ]\n",
      " ...\n",
      " [0.42033303]\n",
      " [0.574109  ]\n",
      " [0.3620147 ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [    6    16    42 ... 28140 28144 28157]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6188\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6485 - acc: 0.6181\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6187\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6197\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6180\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6188\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6482 - acc: 0.6186\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 33us/step - loss: 0.6479 - acc: 0.6184\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6182\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6479 - acc: 0.6198\n",
      "[[0.37404063]\n",
      " [0.27226973]\n",
      " [0.57153124]\n",
      " ...\n",
      " [0.4636049 ]\n",
      " [0.36187223]\n",
      " [0.6104661 ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    1     2     3 ... 28183 28184 28185] TEST: [    0     4     5 ... 28146 28151 28152]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6503 - acc: 0.6167\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6171\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6177\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6163\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6499 - acc: 0.6173\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6175\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6184\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6500 - acc: 0.6183\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6200\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6167\n",
      "[[0.43699506]\n",
      " [0.5025271 ]\n",
      " [0.31738937]\n",
      " ...\n",
      " [0.40311396]\n",
      " [0.29145947]\n",
      " [0.61036146]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [    3    11    12 ... 28153 28167 28175]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6201\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6192\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6209\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6199\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6196\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6210\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6206\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6476 - acc: 0.6195\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6475 - acc: 0.6207\n",
      "[[0.9731317 ]\n",
      " [0.2916953 ]\n",
      " [0.50070006]\n",
      " ...\n",
      " [0.4265527 ]\n",
      " [0.32462734]\n",
      " [0.45833972]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28182 28183 28184] TEST: [    7    19    24 ... 28163 28172 28185]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6213\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6208\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6205\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6214\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6219\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6211\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6219\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6207\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6231\n",
      "[[0.53410596]\n",
      " [0.40245327]\n",
      " [0.42052284]\n",
      " ...\n",
      " [0.36582738]\n",
      " [0.38678405]\n",
      " [0.22569577]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28180 28182 28185] TEST: [    9    26    29 ... 28181 28183 28184]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 32us/step - loss: 0.6494 - acc: 0.6206\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6494 - acc: 0.6207\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6497 - acc: 0.6192\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6493 - acc: 0.6183\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6206\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6496 - acc: 0.6192\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6494 - acc: 0.6192\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6494 - acc: 0.6194\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6495 - acc: 0.6200\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6493 - acc: 0.6221\n",
      "[[0.56167597]\n",
      " [0.3963182 ]\n",
      " [0.6430398 ]\n",
      " ...\n",
      " [0.42368534]\n",
      " [0.44770187]\n",
      " [0.65939415]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "TRAIN: [    0     2     3 ... 28183 28184 28185] TEST: [    1    15    25 ... 28177 28178 28180]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6210\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6207\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6489 - acc: 0.6222\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6211\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6485 - acc: 0.6223\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6210\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6487 - acc: 0.6221\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 33us/step - loss: 0.6486 - acc: 0.6216\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6207\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6233\n",
      "[[0.42804784]\n",
      " [0.32197353]\n",
      " [0.37104788]\n",
      " ...\n",
      " [0.17095874]\n",
      " [0.27996573]\n",
      " [0.563619  ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   10    20    43 ... 28121 28158 28182]\n",
      "Epoch 1/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6500 - acc: 0.6199\n",
      "Epoch 2/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6206\n",
      "Epoch 3/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6196\n",
      "Epoch 4/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6208\n",
      "Epoch 5/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6500 - acc: 0.6195\n",
      "Epoch 6/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6204\n",
      "Epoch 7/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6219\n",
      "Epoch 8/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6499 - acc: 0.6199\n",
      "Epoch 9/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6496 - acc: 0.6202\n",
      "Epoch 10/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6209\n",
      "[[0.33589718]\n",
      " [0.28653806]\n",
      " [0.5136746 ]\n",
      " ...\n",
      " [0.44097656]\n",
      " [0.30075407]\n",
      " [0.42764103]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   28    48    59 ... 28162 28168 28169]\n",
      "Epoch 1/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6487 - acc: 0.6204\n",
      "Epoch 2/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6487 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6486 - acc: 0.6202\n",
      "Epoch 4/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6217\n",
      "Epoch 5/10\n",
      "25369/25369 [==============================] - 1s 34us/step - loss: 0.6483 - acc: 0.6203\n",
      "Epoch 6/10\n",
      "25369/25369 [==============================] - 1s 37us/step - loss: 0.6486 - acc: 0.6211\n",
      "Epoch 7/10\n",
      "25369/25369 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6213\n",
      "Epoch 8/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6482 - acc: 0.6213\n",
      "Epoch 9/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6187\n",
      "Epoch 10/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6480 - acc: 0.6209\n",
      "[[0.56341594]\n",
      " [0.4691869 ]\n",
      " [0.58295834]\n",
      " ...\n",
      " [0.58190924]\n",
      " [0.32308304]\n",
      " [0.34749883]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "batch_size = 16\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "for train, test in kfold.split(x, y):\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train, X_test = x.iloc[train], x.iloc[test]\n",
    "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "\n",
    "    # Fit the model\n",
    "    modelArelu.fit(X_train.values, y_train.values,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "                  )\n",
    "\n",
    "\n",
    "    results = modelArelu.predict(X_test, batch_size=batch_size, verbose=0, steps=None)\n",
    "    \n",
    "    print(results)\n",
    "    print(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2462 - acc: 0.9239 - val_loss: 0.1191 - val_acc: 0.9622\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1045 - acc: 0.9680 - val_loss: 0.0946 - val_acc: 0.9731\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0758 - acc: 0.9772 - val_loss: 0.0811 - val_acc: 0.9764\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0610 - acc: 0.9816 - val_loss: 0.0842 - val_acc: 0.9769\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0517 - acc: 0.9851 - val_loss: 0.0952 - val_acc: 0.9759\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0438 - acc: 0.9865 - val_loss: 0.0853 - val_acc: 0.9783\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0399 - acc: 0.9884 - val_loss: 0.0781 - val_acc: 0.9808\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0354 - acc: 0.9897 - val_loss: 0.0913 - val_acc: 0.9804\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0341 - acc: 0.9901 - val_loss: 0.0804 - val_acc: 0.9816\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0297 - acc: 0.9912 - val_loss: 0.0896 - val_acc: 0.9825\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0264 - acc: 0.9924 - val_loss: 0.0898 - val_acc: 0.9827\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0258 - acc: 0.9928 - val_loss: 0.0885 - val_acc: 0.9818\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0228 - acc: 0.9937 - val_loss: 0.1013 - val_acc: 0.9823\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0221 - acc: 0.9937 - val_loss: 0.1040 - val_acc: 0.9818\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.1135 - val_acc: 0.9823\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.1107 - val_acc: 0.9818\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0197 - acc: 0.9946 - val_loss: 0.0987 - val_acc: 0.9833\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.1132 - val_acc: 0.9829\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0999 - val_acc: 0.9837\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0201 - acc: 0.9949 - val_loss: 0.1079 - val_acc: 0.9832\n",
      "Test loss: 0.10788093121316324\n",
      "Test accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "fold_num = 10\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
