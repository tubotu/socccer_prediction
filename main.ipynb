{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28186\n",
      "          HHATT     HHDEF     HAATT     HADEF     AHATT     AHDEF     AAATT  \\\n",
      "8786   0.923317  0.881482  0.170981  0.943447  2.672943 -0.710455  1.865760   \n",
      "7852  -0.755908  0.496644 -0.145410  0.585298  0.444773 -0.125090  1.022363   \n",
      "2910   0.343985  0.017492 -0.034297  0.373169  0.298621  0.115688 -0.355079   \n",
      "9918   5.664396 -0.118996  1.640708 -0.576552  0.813753  0.008538  0.017193   \n",
      "14322  0.546451 -0.414521  0.604462 -0.338395 -0.021979 -0.187426  0.192316   \n",
      "\n",
      "          AADEF  \n",
      "8786  -0.760125  \n",
      "7852  -0.014479  \n",
      "2910   0.768625  \n",
      "9918   0.996157  \n",
      "14322 -0.133027  \n",
      "       W  D  L\n",
      "8786   0  0  1\n",
      "7852   0  0  1\n",
      "2910   1  0  0\n",
      "9918   1  0  0\n",
      "14322  1  0  0\n",
      "          HHATT     HHDEF     HAATT     HADEF     AHATT     AHDEF     AAATT  \\\n",
      "4356   0.092412 -0.041416 -0.034599 -0.006496  0.162067 -0.051721 -0.037287   \n",
      "26581  0.183477  0.304217 -0.214807  0.489547 -0.122499  0.369468  0.033757   \n",
      "7550  -0.032407  0.726275  0.367550  0.897207  0.107485  0.251516 -0.309932   \n",
      "21212  0.669121 -0.139738  0.877953 -0.074320  0.370077  0.068579  0.158182   \n",
      "7054   0.611987 -0.068185  0.700423 -0.607361 -0.860986  0.165089 -0.417308   \n",
      "\n",
      "          AADEF  \n",
      "4356  -0.061464  \n",
      "26581  0.347818  \n",
      "7550   0.492155  \n",
      "21212  0.068645  \n",
      "7054   0.108434  \n",
      "       W  D  L\n",
      "78710  0  1  0\n",
      "16358  0  1  0\n",
      "12799  0  0  1\n",
      "13386  0  0  1\n",
      "886    0  1  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# data input \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "print(len(data))\n",
    "\n",
    "# Data extraction,Win\n",
    "win_data = data[data.label == \"W\"]\n",
    "win_data = win_data.assign(W=1,D=0,L=0) \n",
    "win_data = win_data.drop(\"label\",axis=1)\n",
    "win_data = win_data.sample(n=len(win_data))# random sort\n",
    "\n",
    "# Data extraction,Draw\n",
    "draw_data = data[data.label == \"D\"]\n",
    "draw_data = draw_data.assign(W=0,D=1,L=0)\n",
    "draw_data = draw_data.drop(\"label\",axis=1)\n",
    "draw_data = draw_data.sample(n=len(draw_data))# random sort\n",
    "\n",
    "# Data extraction,Lose\n",
    "lose_data = data[data.label == \"L\"]\n",
    "lose_data = lose_data.assign(W=0,D=0,L=1) \n",
    "lose_data = lose_data.drop(\"label\",axis=1)\n",
    "lose_data = lose_data.sample(n=len(lose_data))# random sort\n",
    "\n",
    "# Group data together,use for normal k-fold\n",
    "all_data = pd.concat([win_data, draw_data, lose_data])\n",
    "all_data = all_data.sample(n=len(all_data))# random sort\n",
    "#yAll = xAll[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Data drop\n",
    "#xWin = xWin.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xDraw = xDraw.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xLose = xLose.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xAll = xAll.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "\n",
    "# Data separate and making dataset\n",
    "win_data_separate = []\n",
    "draw_data_separate = []\n",
    "lose_data_separate = []\n",
    "all_data_separate = []\n",
    "wdl_separate = []\n",
    "for i in range(10):\n",
    "    win_data_separate.append(win_data[i::10])\n",
    "    draw_data_separate.append(draw_data[i::10])\n",
    "    lose_data_separate.append(lose_data[i::10])\n",
    "    all_data_separate.append(all_data[i::10])\n",
    "    # merge for stratified sampling\n",
    "    wdl_separate.append(pd.concat([win_data_separate[i],draw_data_separate[i],lose_data_separate[i]]))\n",
    "    # assign a number to make final input data\n",
    "    wdl_separate[i] = wdl_separate[i].assign(separate_num=i)\n",
    "    all_data_separate[i] = all_data_separate[i].assign(separate_num=i)\n",
    "\n",
    "# integrate everything once\n",
    "wdl_separate_merge = wdl_separate[0]\n",
    "all_data_separate_merge = all_data_separate[0]\n",
    "for i in range(1,10):\n",
    "    wdl_separate_merge = wdl_separate_merge.append(wdl_separate[i])\n",
    "    all_data_separate_merge = all_data_separate_merge.append(all_data_separate[i])\n",
    "\n",
    "#print(len(wdl_separate_merge))\n",
    "#print(len(all_data_separate_merge))\n",
    "    \n",
    "    \n",
    "# make final input data\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "xAll_train = []\n",
    "yAll_train = []\n",
    "xAll_test = []\n",
    "yAll_test = []\n",
    "for i in range(10):\n",
    "    x_train.append(wdl_separate_merge[wdl_separate_merge['separate_num'] != i])\n",
    "    x_test.append(wdl_separate_merge[wdl_separate_merge['separate_num'] == i])\n",
    "    xAll_train.append(all_data_separate_merge[all_data_separate_merge['separate_num'] != i])\n",
    "    xAll_test.append(all_data_separate_merge[all_data_separate_merge['separate_num'] == i])\n",
    "for i in range(10):\n",
    "    # delete separate_num\n",
    "    x_train[i] = x_train[i].drop(['separate_num'],axis=1)\n",
    "    x_test[i] = x_test[i].drop(['separate_num'],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop(['separate_num'],axis=1)\n",
    "    xAll_test[i] = xAll_test[i].drop(['separate_num'],axis=1)\n",
    "    # random sort\n",
    "    x_train[i] = x_train[i].sample(n=len(x_train[i]))\n",
    "    x_test[i] = x_test[i].sample(n=len(x_test[i]))\n",
    "    xAll_train[i] = xAll_train[i].sample(n=len(xAll_train[i]))\n",
    "    xAll_test[i] = xAll_test[i].sample(n=len(xAll_test[i]))\n",
    "    \n",
    "    # separate x and y\n",
    "    y_train.append(x_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    y_test.append(x_test[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_train.append(xAll_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_test.append(xAll_test[i][[\"W\",\"D\",\"L\"]])\n",
    "    x_train[i] = x_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    x_test[i] = x_test[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_test[i] = xAll_test[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "\n",
    "\n",
    "print(x_train[0].head())\n",
    "print(y_train[0].head())\n",
    "print(xAll_train[0].head())\n",
    "print(yAll_test[0].head())\n",
    "\n",
    "\n",
    "#print(len(y_train[0].query('W == 1')))\n",
    "#print(len(y_train[0].query('L == 1')))\n",
    "#print(len(y_train[0].query('D == 1')))\n",
    "#print(len(yAll_train[0].query('W == 1')))\n",
    "#print(len(yAll_train[0].query('L == 1')))\n",
    "#print(len(yAll_train[0].query('D == 1')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "modelArelu = Sequential()\n",
    "modelArelu.add(Dense(10, activation='relu'))\n",
    "modelArelu.add(Dense(10, activation='relu'))\n",
    "modelArelu.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def RPS(y_true, y_pred):\n",
    "    siz = tf.size(y_true)\n",
    "    cumulative_sum_y_true = 0.\n",
    "    cumulative_sum_y_pred = 0.\n",
    "    output = 0.\n",
    "    for i in siz-1:\n",
    "        cumulative_sum_y_true += y_true[i]\n",
    "        cumulative_sum_y_pred += y_pred[i]\n",
    "        output += (cumulative_sum_y_true - cumulative_sum_y_pred) ** 2\n",
    "        \n",
    "    output /= (siz-1)\n",
    "    return output\n",
    "        \n",
    "\n",
    "modelArelu.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     3 ... 28183 28184 28185] TEST: [    2     8    17 ... 28166 28173 28176]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 37us/step - loss: 0.6569 - acc: 0.6059\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6515 - acc: 0.6147\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6509 - acc: 0.6170\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6506 - acc: 0.6153\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6503 - acc: 0.6142\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6499 - acc: 0.6164\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6500 - acc: 0.6158\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6495 - acc: 0.6173\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6157\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6497 - acc: 0.6158\n",
      "[[0.4044243 ]\n",
      " [0.7105591 ]\n",
      " [0.840841  ]\n",
      " ...\n",
      " [0.43311858]\n",
      " [0.44360483]\n",
      " [0.5276969 ]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   13    21    54 ... 28165 28170 28171]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6494 - acc: 0.6164\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6491 - acc: 0.6196\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6489 - acc: 0.6184\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6490 - acc: 0.6167\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6490 - acc: 0.6185\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6489 - acc: 0.6178\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6488 - acc: 0.6180\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6487 - acc: 0.6190\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6175\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6486 - acc: 0.6188\n",
      "[[0.41986367]\n",
      " [0.4078181 ]\n",
      " [0.4373505 ]\n",
      " ...\n",
      " [0.42033303]\n",
      " [0.574109  ]\n",
      " [0.3620147 ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [    6    16    42 ... 28140 28144 28157]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6188\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6485 - acc: 0.6181\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6187\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6197\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6180\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6188\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6482 - acc: 0.6186\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 33us/step - loss: 0.6479 - acc: 0.6184\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6182\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6479 - acc: 0.6198\n",
      "[[0.37404063]\n",
      " [0.27226973]\n",
      " [0.57153124]\n",
      " ...\n",
      " [0.4636049 ]\n",
      " [0.36187223]\n",
      " [0.6104661 ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    1     2     3 ... 28183 28184 28185] TEST: [    0     4     5 ... 28146 28151 28152]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6503 - acc: 0.6167\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6171\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6177\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6163\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6499 - acc: 0.6173\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6175\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6184\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6500 - acc: 0.6183\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6501 - acc: 0.6200\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6167\n",
      "[[0.43699506]\n",
      " [0.5025271 ]\n",
      " [0.31738937]\n",
      " ...\n",
      " [0.40311396]\n",
      " [0.29145947]\n",
      " [0.61036146]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [    3    11    12 ... 28153 28167 28175]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6201\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6192\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6209\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6199\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6196\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6210\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6206\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6476 - acc: 0.6195\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6475 - acc: 0.6207\n",
      "[[0.9731317 ]\n",
      " [0.2916953 ]\n",
      " [0.50070006]\n",
      " ...\n",
      " [0.4265527 ]\n",
      " [0.32462734]\n",
      " [0.45833972]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28182 28183 28184] TEST: [    7    19    24 ... 28163 28172 28185]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6484 - acc: 0.6213\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6482 - acc: 0.6208\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6481 - acc: 0.6205\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6478 - acc: 0.6214\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6219\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6211\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6219\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6479 - acc: 0.6207\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6477 - acc: 0.6231\n",
      "[[0.53410596]\n",
      " [0.40245327]\n",
      " [0.42052284]\n",
      " ...\n",
      " [0.36582738]\n",
      " [0.38678405]\n",
      " [0.22569577]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28180 28182 28185] TEST: [    9    26    29 ... 28181 28183 28184]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 32us/step - loss: 0.6494 - acc: 0.6206\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6494 - acc: 0.6207\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6497 - acc: 0.6192\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6493 - acc: 0.6183\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6498 - acc: 0.6206\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6496 - acc: 0.6192\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6494 - acc: 0.6192\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6494 - acc: 0.6194\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6495 - acc: 0.6200\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6493 - acc: 0.6221\n",
      "[[0.56167597]\n",
      " [0.3963182 ]\n",
      " [0.6430398 ]\n",
      " ...\n",
      " [0.42368534]\n",
      " [0.44770187]\n",
      " [0.65939415]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n",
      "TRAIN: [    0     2     3 ... 28183 28184 28185] TEST: [    1    15    25 ... 28177 28178 28180]\n",
      "Epoch 1/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6210\n",
      "Epoch 2/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6207\n",
      "Epoch 3/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6489 - acc: 0.6222\n",
      "Epoch 4/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6211\n",
      "Epoch 5/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6485 - acc: 0.6223\n",
      "Epoch 6/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6488 - acc: 0.6210\n",
      "Epoch 7/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6487 - acc: 0.6221\n",
      "Epoch 8/10\n",
      "25367/25367 [==============================] - 1s 33us/step - loss: 0.6486 - acc: 0.6216\n",
      "Epoch 9/10\n",
      "25367/25367 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6207\n",
      "Epoch 10/10\n",
      "25367/25367 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6233\n",
      "[[0.42804784]\n",
      " [0.32197353]\n",
      " [0.37104788]\n",
      " ...\n",
      " [0.17095874]\n",
      " [0.27996573]\n",
      " [0.563619  ]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   10    20    43 ... 28121 28158 28182]\n",
      "Epoch 1/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6500 - acc: 0.6199\n",
      "Epoch 2/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6206\n",
      "Epoch 3/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6196\n",
      "Epoch 4/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6208\n",
      "Epoch 5/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6500 - acc: 0.6195\n",
      "Epoch 6/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6204\n",
      "Epoch 7/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6498 - acc: 0.6219\n",
      "Epoch 8/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6499 - acc: 0.6199\n",
      "Epoch 9/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6496 - acc: 0.6202\n",
      "Epoch 10/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6501 - acc: 0.6209\n",
      "[[0.33589718]\n",
      " [0.28653806]\n",
      " [0.5136746 ]\n",
      " ...\n",
      " [0.44097656]\n",
      " [0.30075407]\n",
      " [0.42764103]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "TRAIN: [    0     1     2 ... 28183 28184 28185] TEST: [   28    48    59 ... 28162 28168 28169]\n",
      "Epoch 1/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6487 - acc: 0.6204\n",
      "Epoch 2/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6487 - acc: 0.6210\n",
      "Epoch 3/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6486 - acc: 0.6202\n",
      "Epoch 4/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6217\n",
      "Epoch 5/10\n",
      "25369/25369 [==============================] - 1s 34us/step - loss: 0.6483 - acc: 0.6203\n",
      "Epoch 6/10\n",
      "25369/25369 [==============================] - 1s 37us/step - loss: 0.6486 - acc: 0.6211\n",
      "Epoch 7/10\n",
      "25369/25369 [==============================] - 1s 31us/step - loss: 0.6483 - acc: 0.6213\n",
      "Epoch 8/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6482 - acc: 0.6213\n",
      "Epoch 9/10\n",
      "25369/25369 [==============================] - 1s 30us/step - loss: 0.6484 - acc: 0.6187\n",
      "Epoch 10/10\n",
      "25369/25369 [==============================] - 1s 29us/step - loss: 0.6480 - acc: 0.6209\n",
      "[[0.56341594]\n",
      " [0.4691869 ]\n",
      " [0.58295834]\n",
      " ...\n",
      " [0.58190924]\n",
      " [0.32308304]\n",
      " [0.34749883]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "cvscores = []\n",
    "batch_size = 16\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "for train, test in kfold.split(x, y):\n",
    "    print(\"TRAIN:\", train, \"TEST:\", test)\n",
    "    X_train, X_test = x.iloc[train], x.iloc[test]\n",
    "    y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "\n",
    "    # Fit the model\n",
    "    modelArelu.fit(X_train.values, y_train.values,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1\n",
    "                  )\n",
    "\n",
    "\n",
    "    results = modelArelu.predict(X_test, batch_size=batch_size, verbose=0, steps=None)\n",
    "    \n",
    "    print(results)\n",
    "    print(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/eggplant2/rintaro/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2462 - acc: 0.9239 - val_loss: 0.1191 - val_acc: 0.9622\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1045 - acc: 0.9680 - val_loss: 0.0946 - val_acc: 0.9731\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0758 - acc: 0.9772 - val_loss: 0.0811 - val_acc: 0.9764\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0610 - acc: 0.9816 - val_loss: 0.0842 - val_acc: 0.9769\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0517 - acc: 0.9851 - val_loss: 0.0952 - val_acc: 0.9759\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0438 - acc: 0.9865 - val_loss: 0.0853 - val_acc: 0.9783\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0399 - acc: 0.9884 - val_loss: 0.0781 - val_acc: 0.9808\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0354 - acc: 0.9897 - val_loss: 0.0913 - val_acc: 0.9804\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0341 - acc: 0.9901 - val_loss: 0.0804 - val_acc: 0.9816\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0297 - acc: 0.9912 - val_loss: 0.0896 - val_acc: 0.9825\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0264 - acc: 0.9924 - val_loss: 0.0898 - val_acc: 0.9827\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0258 - acc: 0.9928 - val_loss: 0.0885 - val_acc: 0.9818\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0228 - acc: 0.9937 - val_loss: 0.1013 - val_acc: 0.9823\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0221 - acc: 0.9937 - val_loss: 0.1040 - val_acc: 0.9818\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.1135 - val_acc: 0.9823\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.1107 - val_acc: 0.9818\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0197 - acc: 0.9946 - val_loss: 0.0987 - val_acc: 0.9833\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.1132 - val_acc: 0.9829\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0999 - val_acc: 0.9837\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0201 - acc: 0.9949 - val_loss: 0.1079 - val_acc: 0.9832\n",
      "Test loss: 0.10788093121316324\n",
      "Test accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "fold_num = 10\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
