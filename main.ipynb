{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28186\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "split_num = 10\n",
    "\n",
    "\n",
    "# data input \n",
    "path = 'trainrat_new.txt'\n",
    "data = pd.read_csv(path,sep=' ')\n",
    "print(len(data))\n",
    "\n",
    "# Data extraction,Win\n",
    "win_data = data[data.label == \"W\"]\n",
    "win_data = win_data.assign(W=1,D=0,L=0) \n",
    "win_data = win_data.drop(\"label\",axis=1)\n",
    "win_data = win_data.sample(n=len(win_data))# random sort\n",
    "\n",
    "# Data extraction,Draw\n",
    "draw_data = data[data.label == \"D\"]\n",
    "draw_data = draw_data.assign(W=0,D=1,L=0)\n",
    "draw_data = draw_data.drop(\"label\",axis=1)\n",
    "draw_data = draw_data.sample(n=len(draw_data))# random sort\n",
    "\n",
    "# Data extraction,Lose\n",
    "lose_data = data[data.label == \"L\"]\n",
    "lose_data = lose_data.assign(W=0,D=0,L=1) \n",
    "lose_data = lose_data.drop(\"label\",axis=1)\n",
    "lose_data = lose_data.sample(n=len(lose_data))# random sort\n",
    "\n",
    "# Group data together,use for normal k-fold\n",
    "all_data = pd.concat([win_data, draw_data, lose_data])\n",
    "all_data = all_data.sample(n=len(all_data))# random sort\n",
    "#yAll = xAll[[\"W\",\"D\",\"L\"]]\n",
    "\n",
    "# Data drop\n",
    "#xWin = xWin.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xDraw = xDraw.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xLose = xLose.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "#xAll = xAll.drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "\n",
    "# Data separate and making dataset\n",
    "win_data_separate = []\n",
    "draw_data_separate = []\n",
    "lose_data_separate = []\n",
    "all_data_separate = []\n",
    "wdl_separate = []\n",
    "for i in range(split_num):\n",
    "    win_data_separate.append(win_data[i::split_num])\n",
    "    draw_data_separate.append(draw_data[i::split_num])\n",
    "    lose_data_separate.append(lose_data[i::split_num])\n",
    "    all_data_separate.append(all_data[i::split_num])\n",
    "    # merge for stratified sampling\n",
    "    wdl_separate.append(pd.concat([win_data_separate[i],draw_data_separate[i],lose_data_separate[i]]))\n",
    "    # assign a number to make final input data\n",
    "    wdl_separate[i] = wdl_separate[i].assign(separate_num=i)\n",
    "    all_data_separate[i] = all_data_separate[i].assign(separate_num=i)\n",
    "\n",
    "# integrate everything once\n",
    "wdl_separate_merge = wdl_separate[0]\n",
    "all_data_separate_merge = all_data_separate[0]\n",
    "for i in range(1,split_num):\n",
    "    wdl_separate_merge = wdl_separate_merge.append(wdl_separate[i])\n",
    "    all_data_separate_merge = all_data_separate_merge.append(all_data_separate[i])\n",
    "\n",
    "#print(len(wdl_separate_merge))\n",
    "#print(len(all_data_separate_merge))\n",
    "    \n",
    "    \n",
    "# make final input data\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "xAll_train = []\n",
    "yAll_train = []\n",
    "xAll_val = []\n",
    "yAll_val = []\n",
    "for i in range(split_num):\n",
    "    x_train.append(wdl_separate_merge[wdl_separate_merge['separate_num'] != i])\n",
    "    x_val.append(wdl_separate_merge[wdl_separate_merge['separate_num'] == i])\n",
    "    xAll_train.append(all_data_separate_merge[all_data_separate_merge['separate_num'] != i])\n",
    "    xAll_val.append(all_data_separate_merge[all_data_separate_merge['separate_num'] == i])\n",
    "for i in range(split_num):\n",
    "    # delete separate_num\n",
    "    x_train[i] = x_train[i].drop(['separate_num'],axis=1)\n",
    "    x_val[i] = x_val[i].drop(['separate_num'],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop(['separate_num'],axis=1)\n",
    "    xAll_val[i] = xAll_val[i].drop(['separate_num'],axis=1)\n",
    "    # random sort\n",
    "    x_train[i] = x_train[i].sample(n=len(x_train[i]))\n",
    "    x_val[i] = x_val[i].sample(n=len(x_val[i]))\n",
    "    xAll_train[i] = xAll_train[i].sample(n=len(xAll_train[i]))\n",
    "    xAll_val[i] = xAll_val[i].sample(n=len(xAll_val[i]))\n",
    "    \n",
    "    # separate x and y\n",
    "    y_train.append(x_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    y_val.append(x_val[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_train.append(xAll_train[i][[\"W\",\"D\",\"L\"]])\n",
    "    yAll_val.append(xAll_val[i][[\"W\",\"D\",\"L\"]])\n",
    "    x_train[i] = x_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    x_val[i] = x_val[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_train[i] = xAll_train[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    xAll_val[i] = xAll_val[i].drop([\"W\",\"D\",\"L\"],axis=1)\n",
    "    \n",
    "    #translate pandas to numpy\n",
    "    x_train[i] = x_train[i].values.astype('float32') \n",
    "    x_val[i] = x_val[i].values.astype('float32') \n",
    "    y_train[i] = y_train[i].values\n",
    "    y_val[i] = y_val[i].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPS(y_true, y_pred):\n",
    "    output = 0.\n",
    "    data_num = len(y_true)\n",
    "    for i in range(data_num):\n",
    "        times = len(y_true[i]) - 1 \n",
    "        cumulative_sum = 0.\n",
    "        score = 0.\n",
    "        for time in range(times):\n",
    "            cumulative_sum += y_true[i,time] - y_pred[i,time]\n",
    "            score += cumulative_sum ** 2\n",
    "        score /= times\n",
    "        output += score\n",
    "    \n",
    "    output /= data_num\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tubotu/.local/lib/python3.6/site-packages/chainer/backends/cuda.py:143: UserWarning: cuDNN is not enabled.\n",
      "Please reinstall CuPy after you install cudnn\n",
      "(see https://docs-cupy.chainer.org/en/stable/install.html#install-cudnn).\n",
      "  'cuDNN is not enabled.\\n'\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Sequential\n",
    "\n",
    "# craete model\n",
    "# net としてインスタンス化\n",
    "n_input = 8\n",
    "n_hidden = 10\n",
    "n_output = 3\n",
    "\n",
    "# create 10 model for 10-fold-crossvalidation?\n",
    "optimizer = []\n",
    "net = []\n",
    "\n",
    "for i in range(10):\n",
    "    net.append(Sequential(\n",
    "        L.Linear(n_input, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_hidden), F.relu,\n",
    "        L.Linear(n_hidden, n_output), F.softmax)\n",
    "    )\n",
    "    optimizer.append(chainer.optimizers.SGD(lr=0.01))\n",
    "    optimizer[i].setup(net[i])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 1586, loss (train): 0.2223, loss (valid): 0.2183\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2164, loss (valid): 0.2159\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2147, loss (valid): 0.2143\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2135, loss (valid): 0.2130\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2125, loss (valid): 0.2121\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2118, loss (valid): 0.2115\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2113, loss (valid): 0.2110\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2109, loss (valid): 0.2107\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2107, loss (valid): 0.2105\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2105, loss (valid): 0.2103\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2104, loss (valid): 0.2101\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2102, loss (valid): 0.2100\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2101, loss (valid): 0.2098\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2100, loss (valid): 0.2097\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2099, loss (valid): 0.2096\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2099, loss (valid): 0.2096\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2098, loss (valid): 0.2095\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2097, loss (valid): 0.2094\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2097, loss (valid): 0.2093\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2096, loss (valid): 0.2093\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2096, loss (valid): 0.2092\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2095, loss (valid): 0.2092\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2095, loss (valid): 0.2091\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2095, loss (valid): 0.2091\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2094, loss (valid): 0.2090\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2094, loss (valid): 0.2090\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2094, loss (valid): 0.2089\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2094, loss (valid): 0.2089\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2093, loss (valid): 0.2089\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2093, loss (valid): 0.2089\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2093, loss (valid): 0.2088\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2093, loss (valid): 0.2088\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2093, loss (valid): 0.2088\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2092, loss (valid): 0.2087\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2092, loss (valid): 0.2087\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2092, loss (valid): 0.2087\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2092, loss (valid): 0.2087\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2092, loss (valid): 0.2087\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2091, loss (valid): 0.2086\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2091, loss (valid): 0.2085\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2091, loss (valid): 0.2085\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2091, loss (valid): 0.2085\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2091, loss (valid): 0.2085\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2090, loss (valid): 0.2085\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2090, loss (valid): 0.2084\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2089, loss (valid): 0.2084\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2089, loss (valid): 0.2083\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2088, loss (valid): 0.2083\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2087, loss (valid): 0.2083\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2279, loss (valid): 0.2224\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2201, loss (valid): 0.2179\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2166, loss (valid): 0.2153\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2144, loss (valid): 0.2136\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2129, loss (valid): 0.2123\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2118, loss (valid): 0.2114\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2111, loss (valid): 0.2108\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2106, loss (valid): 0.2104\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2103, loss (valid): 0.2101\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2100, loss (valid): 0.2099\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2099, loss (valid): 0.2098\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2098, loss (valid): 0.2097\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2097, loss (valid): 0.2096\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2096, loss (valid): 0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, iteration: 23790, loss (train): 0.2095, loss (valid): 0.2094\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2095, loss (valid): 0.2093\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2094, loss (valid): 0.2093\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2094, loss (valid): 0.2092\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2093, loss (valid): 0.2092\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2093, loss (valid): 0.2092\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2093, loss (valid): 0.2091\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2092, loss (valid): 0.2091\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2092, loss (valid): 0.2091\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2092, loss (valid): 0.2090\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2092, loss (valid): 0.2090\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2091, loss (valid): 0.2090\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2091, loss (valid): 0.2089\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2091, loss (valid): 0.2089\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2091, loss (valid): 0.2089\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2091, loss (valid): 0.2089\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2091, loss (valid): 0.2089\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2089, loss (valid): 0.2086\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2088, loss (valid): 0.2085\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2086, loss (valid): 0.2084\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2260, loss (valid): 0.2211\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2205, loss (valid): 0.2184\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2184, loss (valid): 0.2161\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2164, loss (valid): 0.2139\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2146, loss (valid): 0.2121\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2132, loss (valid): 0.2110\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2123, loss (valid): 0.2102\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2118, loss (valid): 0.2098\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2114, loss (valid): 0.2095\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2111, loss (valid): 0.2093\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2109, loss (valid): 0.2091\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2107, loss (valid): 0.2090\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2105, loss (valid): 0.2089\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2104, loss (valid): 0.2088\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2102, loss (valid): 0.2087\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2101, loss (valid): 0.2086\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2100, loss (valid): 0.2085\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2099, loss (valid): 0.2085\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2099, loss (valid): 0.2084\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2098, loss (valid): 0.2084\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2097, loss (valid): 0.2084\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2097, loss (valid): 0.2084\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2095, loss (valid): 0.2083\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2095, loss (valid): 0.2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, iteration: 45994, loss (train): 0.2094, loss (valid): 0.2082\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2094, loss (valid): 0.2082\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2094, loss (valid): 0.2082\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2093, loss (valid): 0.2081\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2092, loss (valid): 0.2081\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2092, loss (valid): 0.2081\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2092, loss (valid): 0.2081\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2092, loss (valid): 0.2081\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2092, loss (valid): 0.2081\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2091, loss (valid): 0.2081\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2090, loss (valid): 0.2081\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2089, loss (valid): 0.2081\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2088, loss (valid): 0.2081\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2087, loss (valid): 0.2081\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2086, loss (valid): 0.2081\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2279, loss (valid): 0.2220\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2185, loss (valid): 0.2157\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2141, loss (valid): 0.2126\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2122, loss (valid): 0.2112\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2114, loss (valid): 0.2105\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2109, loss (valid): 0.2100\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2106, loss (valid): 0.2097\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2103, loss (valid): 0.2094\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2102, loss (valid): 0.2093\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2100, loss (valid): 0.2092\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2099, loss (valid): 0.2091\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2098, loss (valid): 0.2091\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2097, loss (valid): 0.2090\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2097, loss (valid): 0.2090\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2096, loss (valid): 0.2090\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2095, loss (valid): 0.2089\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2095, loss (valid): 0.2089\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2095, loss (valid): 0.2089\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2094, loss (valid): 0.2089\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2094, loss (valid): 0.2089\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2093, loss (valid): 0.2089\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2093, loss (valid): 0.2088\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2093, loss (valid): 0.2088\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2092, loss (valid): 0.2088\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2091, loss (valid): 0.2088\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2090, loss (valid): 0.2088\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2090, loss (valid): 0.2087\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2089, loss (valid): 0.2087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42, iteration: 68198, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2089, loss (valid): 0.2087\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2088, loss (valid): 0.2087\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2088, loss (valid): 0.2086\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2087, loss (valid): 0.2086\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2087, loss (valid): 0.2085\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2086, loss (valid): 0.2085\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2085, loss (valid): 0.2085\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2085, loss (valid): 0.2085\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2085, loss (valid): 0.2085\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2085, loss (valid): 0.2085\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2085, loss (valid): 0.2085\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2085, loss (valid): 0.2084\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2193, loss (valid): 0.2138\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2135, loss (valid): 0.2102\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2117, loss (valid): 0.2093\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2111, loss (valid): 0.2089\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2108, loss (valid): 0.2087\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2106, loss (valid): 0.2085\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2104, loss (valid): 0.2084\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2103, loss (valid): 0.2083\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2101, loss (valid): 0.2082\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2100, loss (valid): 0.2081\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2099, loss (valid): 0.2081\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2098, loss (valid): 0.2080\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2098, loss (valid): 0.2080\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2097, loss (valid): 0.2079\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2096, loss (valid): 0.2079\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2096, loss (valid): 0.2079\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2095, loss (valid): 0.2078\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2095, loss (valid): 0.2078\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2095, loss (valid): 0.2078\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2094, loss (valid): 0.2078\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2094, loss (valid): 0.2078\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2094, loss (valid): 0.2077\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2094, loss (valid): 0.2077\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2093, loss (valid): 0.2077\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2093, loss (valid): 0.2077\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2093, loss (valid): 0.2077\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2093, loss (valid): 0.2076\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2093, loss (valid): 0.2076\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2092, loss (valid): 0.2076\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2091, loss (valid): 0.2076\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2090, loss (valid): 0.2076\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2089, loss (valid): 0.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 56, iteration: 90402, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2089, loss (valid): 0.2076\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2088, loss (valid): 0.2076\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2088, loss (valid): 0.2077\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2087, loss (valid): 0.2077\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2086, loss (valid): 0.2077\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2294, loss (valid): 0.2222\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2208, loss (valid): 0.2189\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2172, loss (valid): 0.2158\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2139, loss (valid): 0.2138\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2119, loss (valid): 0.2132\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2111, loss (valid): 0.2130\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2107, loss (valid): 0.2128\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2105, loss (valid): 0.2127\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2103, loss (valid): 0.2126\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2101, loss (valid): 0.2125\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2100, loss (valid): 0.2124\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2099, loss (valid): 0.2123\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2098, loss (valid): 0.2122\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2097, loss (valid): 0.2121\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2096, loss (valid): 0.2120\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2096, loss (valid): 0.2120\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2095, loss (valid): 0.2119\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2094, loss (valid): 0.2119\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2094, loss (valid): 0.2118\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2093, loss (valid): 0.2118\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2093, loss (valid): 0.2118\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2093, loss (valid): 0.2118\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2092, loss (valid): 0.2117\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2092, loss (valid): 0.2117\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2092, loss (valid): 0.2117\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2091, loss (valid): 0.2117\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2091, loss (valid): 0.2116\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2091, loss (valid): 0.2116\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2091, loss (valid): 0.2116\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2090, loss (valid): 0.2116\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2090, loss (valid): 0.2116\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2090, loss (valid): 0.2116\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2090, loss (valid): 0.2116\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2090, loss (valid): 0.2116\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2089, loss (valid): 0.2116\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2089, loss (valid): 0.2116\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2089, loss (valid): 0.2115\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2089, loss (valid): 0.2115\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2089, loss (valid): 0.2115\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2089, loss (valid): 0.2115\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2089, loss (valid): 0.2115\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2088, loss (valid): 0.2115\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2087, loss (valid): 0.2115\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2086, loss (valid): 0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70, iteration: 112606, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2086, loss (valid): 0.2115\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2085, loss (valid): 0.2114\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2084, loss (valid): 0.2114\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2084, loss (valid): 0.2114\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2286, loss (valid): 0.2204\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2174, loss (valid): 0.2145\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2136, loss (valid): 0.2121\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2121, loss (valid): 0.2111\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2113, loss (valid): 0.2107\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2109, loss (valid): 0.2104\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2106, loss (valid): 0.2102\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2104, loss (valid): 0.2101\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2102, loss (valid): 0.2100\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2100, loss (valid): 0.2100\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2099, loss (valid): 0.2099\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2098, loss (valid): 0.2099\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2097, loss (valid): 0.2099\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2096, loss (valid): 0.2098\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2095, loss (valid): 0.2098\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2095, loss (valid): 0.2098\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2094, loss (valid): 0.2098\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2094, loss (valid): 0.2097\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2093, loss (valid): 0.2097\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2093, loss (valid): 0.2097\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2093, loss (valid): 0.2097\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2092, loss (valid): 0.2096\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2092, loss (valid): 0.2096\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2092, loss (valid): 0.2096\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2091, loss (valid): 0.2096\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2091, loss (valid): 0.2095\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2091, loss (valid): 0.2095\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2091, loss (valid): 0.2095\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2090, loss (valid): 0.2095\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2090, loss (valid): 0.2094\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2090, loss (valid): 0.2094\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2090, loss (valid): 0.2094\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2089, loss (valid): 0.2094\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2089, loss (valid): 0.2093\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2089, loss (valid): 0.2093\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2089, loss (valid): 0.2093\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2089, loss (valid): 0.2093\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2088, loss (valid): 0.2093\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2087, loss (valid): 0.2092\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2086, loss (valid): 0.2092\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2085, loss (valid): 0.2092\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2085, loss (valid): 0.2092\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2085, loss (valid): 0.2091\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2084, loss (valid): 0.2091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 84, iteration: 134810, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2084, loss (valid): 0.2091\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2084, loss (valid): 0.2090\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2084, loss (valid): 0.2090\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2084, loss (valid): 0.2090\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2084, loss (valid): 0.2090\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2157, loss (valid): 0.2180\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2120, loss (valid): 0.2169\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2110, loss (valid): 0.2161\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2104, loss (valid): 0.2156\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2101, loss (valid): 0.2153\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2098, loss (valid): 0.2150\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2096, loss (valid): 0.2148\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2095, loss (valid): 0.2147\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2093, loss (valid): 0.2145\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2092, loss (valid): 0.2144\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2092, loss (valid): 0.2143\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2091, loss (valid): 0.2142\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2090, loss (valid): 0.2141\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2090, loss (valid): 0.2141\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2089, loss (valid): 0.2140\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2088, loss (valid): 0.2140\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2088, loss (valid): 0.2139\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2088, loss (valid): 0.2139\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2087, loss (valid): 0.2139\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2087, loss (valid): 0.2138\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2087, loss (valid): 0.2138\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2086, loss (valid): 0.2138\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2086, loss (valid): 0.2138\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2086, loss (valid): 0.2138\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2085, loss (valid): 0.2137\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2085, loss (valid): 0.2137\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2085, loss (valid): 0.2137\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2085, loss (valid): 0.2137\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2084, loss (valid): 0.2137\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2084, loss (valid): 0.2137\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2084, loss (valid): 0.2137\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2084, loss (valid): 0.2137\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2084, loss (valid): 0.2137\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2083, loss (valid): 0.2136\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2083, loss (valid): 0.2136\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2083, loss (valid): 0.2136\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2083, loss (valid): 0.2136\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2083, loss (valid): 0.2136\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2082, loss (valid): 0.2136\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2082, loss (valid): 0.2136\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2082, loss (valid): 0.2136\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2082, loss (valid): 0.2135\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2082, loss (valid): 0.2135\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2082, loss (valid): 0.2135\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2082, loss (valid): 0.2135\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2081, loss (valid): 0.2135\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2081, loss (valid): 0.2134\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2080, loss (valid): 0.2134\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2079, loss (valid): 0.2134\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2078, loss (valid): 0.2134\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2078, loss (valid): 0.2134\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2078, loss (valid): 0.2134\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2078, loss (valid): 0.2133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 98, iteration: 157014, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2078, loss (valid): 0.2133\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2174, loss (valid): 0.2140\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2128, loss (valid): 0.2121\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2116, loss (valid): 0.2113\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2110, loss (valid): 0.2108\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2107, loss (valid): 0.2105\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2104, loss (valid): 0.2103\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2102, loss (valid): 0.2102\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2101, loss (valid): 0.2101\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2099, loss (valid): 0.2100\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2098, loss (valid): 0.2099\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2098, loss (valid): 0.2099\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2097, loss (valid): 0.2098\n",
      "epoch: 12, iteration: 20618, loss (train): 0.2096, loss (valid): 0.2098\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2096, loss (valid): 0.2097\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2095, loss (valid): 0.2097\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2095, loss (valid): 0.2096\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2094, loss (valid): 0.2096\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2094, loss (valid): 0.2096\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2094, loss (valid): 0.2096\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2093, loss (valid): 0.2095\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2093, loss (valid): 0.2095\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2093, loss (valid): 0.2095\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2093, loss (valid): 0.2095\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2092, loss (valid): 0.2094\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2091, loss (valid): 0.2093\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2090, loss (valid): 0.2092\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2089, loss (valid): 0.2092\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2089, loss (valid): 0.2091\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2088, loss (valid): 0.2091\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2088, loss (valid): 0.2090\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2087, loss (valid): 0.2090\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2086, loss (valid): 0.2090\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2086, loss (valid): 0.2090\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2086, loss (valid): 0.2089\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2085, loss (valid): 0.2089\n",
      "epoch: 0, iteration: 1586, loss (train): 0.2207, loss (valid): 0.2181\n",
      "epoch: 1, iteration: 3172, loss (train): 0.2174, loss (valid): 0.2155\n",
      "epoch: 2, iteration: 4758, loss (train): 0.2155, loss (valid): 0.2138\n",
      "epoch: 3, iteration: 6344, loss (train): 0.2142, loss (valid): 0.2126\n",
      "epoch: 4, iteration: 7930, loss (train): 0.2132, loss (valid): 0.2118\n",
      "epoch: 5, iteration: 9516, loss (train): 0.2125, loss (valid): 0.2111\n",
      "epoch: 6, iteration: 11102, loss (train): 0.2120, loss (valid): 0.2106\n",
      "epoch: 7, iteration: 12688, loss (train): 0.2116, loss (valid): 0.2102\n",
      "epoch: 8, iteration: 14274, loss (train): 0.2113, loss (valid): 0.2099\n",
      "epoch: 9, iteration: 15860, loss (train): 0.2110, loss (valid): 0.2096\n",
      "epoch: 10, iteration: 17446, loss (train): 0.2108, loss (valid): 0.2094\n",
      "epoch: 11, iteration: 19032, loss (train): 0.2106, loss (valid): 0.2092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, iteration: 20618, loss (train): 0.2104, loss (valid): 0.2091\n",
      "epoch: 13, iteration: 22204, loss (train): 0.2103, loss (valid): 0.2090\n",
      "epoch: 14, iteration: 23790, loss (train): 0.2102, loss (valid): 0.2089\n",
      "epoch: 15, iteration: 25376, loss (train): 0.2101, loss (valid): 0.2088\n",
      "epoch: 16, iteration: 26962, loss (train): 0.2100, loss (valid): 0.2087\n",
      "epoch: 17, iteration: 28548, loss (train): 0.2099, loss (valid): 0.2086\n",
      "epoch: 18, iteration: 30134, loss (train): 0.2099, loss (valid): 0.2086\n",
      "epoch: 19, iteration: 31720, loss (train): 0.2098, loss (valid): 0.2085\n",
      "epoch: 20, iteration: 33306, loss (train): 0.2098, loss (valid): 0.2085\n",
      "epoch: 21, iteration: 34892, loss (train): 0.2097, loss (valid): 0.2084\n",
      "epoch: 22, iteration: 36478, loss (train): 0.2097, loss (valid): 0.2084\n",
      "epoch: 23, iteration: 38064, loss (train): 0.2097, loss (valid): 0.2084\n",
      "epoch: 24, iteration: 39650, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 25, iteration: 41236, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 26, iteration: 42822, loss (train): 0.2096, loss (valid): 0.2083\n",
      "epoch: 27, iteration: 44408, loss (train): 0.2095, loss (valid): 0.2083\n",
      "epoch: 28, iteration: 45994, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 29, iteration: 47580, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 30, iteration: 49166, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 31, iteration: 50752, loss (train): 0.2095, loss (valid): 0.2082\n",
      "epoch: 32, iteration: 52338, loss (train): 0.2094, loss (valid): 0.2082\n",
      "epoch: 33, iteration: 53924, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 34, iteration: 55510, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 35, iteration: 57096, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 36, iteration: 58682, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 37, iteration: 60268, loss (train): 0.2094, loss (valid): 0.2081\n",
      "epoch: 38, iteration: 61854, loss (train): 0.2094, loss (valid): 0.2080\n",
      "epoch: 39, iteration: 63440, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 40, iteration: 65026, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 41, iteration: 66612, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 42, iteration: 68198, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 43, iteration: 69784, loss (train): 0.2093, loss (valid): 0.2080\n",
      "epoch: 44, iteration: 71370, loss (train): 0.2093, loss (valid): 0.2079\n",
      "epoch: 45, iteration: 72956, loss (train): 0.2093, loss (valid): 0.2079\n",
      "epoch: 46, iteration: 74542, loss (train): 0.2093, loss (valid): 0.2079\n",
      "epoch: 47, iteration: 76128, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 48, iteration: 77714, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 49, iteration: 79300, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 50, iteration: 80886, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 51, iteration: 82472, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 52, iteration: 84058, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 53, iteration: 85644, loss (train): 0.2092, loss (valid): 0.2079\n",
      "epoch: 54, iteration: 87230, loss (train): 0.2092, loss (valid): 0.2078\n",
      "epoch: 55, iteration: 88816, loss (train): 0.2092, loss (valid): 0.2078\n",
      "epoch: 56, iteration: 90402, loss (train): 0.2092, loss (valid): 0.2078\n",
      "epoch: 57, iteration: 91988, loss (train): 0.2092, loss (valid): 0.2078\n",
      "epoch: 58, iteration: 93574, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 59, iteration: 95160, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 60, iteration: 96746, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 61, iteration: 98332, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 62, iteration: 99918, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 63, iteration: 101504, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 64, iteration: 103090, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 65, iteration: 104676, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 66, iteration: 106262, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 67, iteration: 107848, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 68, iteration: 109434, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 69, iteration: 111020, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 70, iteration: 112606, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 71, iteration: 114192, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 72, iteration: 115778, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 73, iteration: 117364, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 74, iteration: 118950, loss (train): 0.2091, loss (valid): 0.2078\n",
      "epoch: 75, iteration: 120536, loss (train): 0.2090, loss (valid): 0.2078\n",
      "epoch: 76, iteration: 122122, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 77, iteration: 123708, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 78, iteration: 125294, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 79, iteration: 126880, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 80, iteration: 128466, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 81, iteration: 130052, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 82, iteration: 131638, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 83, iteration: 133224, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 84, iteration: 134810, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 85, iteration: 136396, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 86, iteration: 137982, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 87, iteration: 139568, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 88, iteration: 141154, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 89, iteration: 142740, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 90, iteration: 144326, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 91, iteration: 145912, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 92, iteration: 147498, loss (train): 0.2090, loss (valid): 0.2077\n",
      "epoch: 93, iteration: 149084, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 94, iteration: 150670, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 95, iteration: 152256, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 96, iteration: 153842, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 97, iteration: 155428, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 98, iteration: 157014, loss (train): 0.2089, loss (valid): 0.2077\n",
      "epoch: 99, iteration: 158600, loss (train): 0.2089, loss (valid): 0.2077\n"
     ]
    }
   ],
   "source": [
    "#print(np.random.permutation(range(len(x_train))))\n",
    "n_epoch = 100\n",
    "batchsize = 16\n",
    "# ログの保存用\n",
    "results_train_data = []\n",
    "results_valid_data = []\n",
    "\n",
    "\n",
    "for data_num in range(len(x_train)):\n",
    "    # ログの保存用\n",
    "    results_train = {\n",
    "        'loss': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "    results_valid = {\n",
    "        'loss': [],\n",
    "        'accuracy': []\n",
    "    }\n",
    "    iteration = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        # 各バッチ毎の目的関数の出力と分類精度の保存用\n",
    "        loss_list = []\n",
    "        #accuracy_list = []\n",
    "\n",
    "        for i in range(0, len(x_train[data_num]), batchsize):\n",
    "            # バッチを準備\n",
    "            x_train_batch = x_train[data_num][i:i+batchsize,:]\n",
    "            y_train_batch = y_train[data_num][i:i+batchsize,:]\n",
    "\n",
    "            # 予測値を出力\n",
    "            y_train_batch_pred = net[data_num](x_train_batch)\n",
    "            # 目的関数を適用し、分類精度を計算\n",
    "            loss_train_batch = RPS(y_train_batch, y_train_batch_pred)\n",
    "            #print(loss_train_batch)\n",
    "            #accuracy_train_batch = F.accuracy(y_train_batch_pred, y_train_batch)\n",
    "\n",
    "            loss_list.append(loss_train_batch.array)\n",
    "            #accuracy_list.append(accuracy_train_batch.array)\n",
    "\n",
    "            # 勾配のリセットと勾配の計算\n",
    "            net[data_num].cleargrads()\n",
    "            loss_train_batch.backward()\n",
    "\n",
    "            # パラメータの更新\n",
    "            optimizer[data_num].update()\n",
    "\n",
    "            # カウントアップ\n",
    "            iteration += 1\n",
    "        # 訓練データに対する目的関数の出力と分類精度を集計\n",
    "        loss_train = np.mean(loss_list)\n",
    "        #accuracy_train = np.mean(accuracy_list)\n",
    "\n",
    "        # 1エポック終えたら、検証データで評価\n",
    "        # 検証データで予測値を出力\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "            y_val_pred = net[data_num](x_val[data_num])\n",
    "\n",
    "        #print(y_val)\n",
    "        # 目的関数を適用し、分類精度を計算\n",
    "        loss_val = RPS(y_val_pred, y_val[data_num])\n",
    "        #accuracy_val = F.accuracy(y_val_pred, y_val[data_num])\n",
    "\n",
    "        # 結果の表示\n",
    "        print('epoch: {}, iteration: {}, loss (train): {:.4f}, loss (valid): {:.4f}'.format(\n",
    "            epoch, iteration, loss_train, loss_val.array))\n",
    "\n",
    "        # ログを保存\n",
    "        results_train['loss'] .append(loss_train)\n",
    "        #results_train['accuracy'] .append(accuracy_train)\n",
    "        results_valid['loss'].append(loss_val.array)\n",
    "        #results_valid['accuracy'].append(accuracy_val.array)\n",
    "        \n",
    "    results_train_data.append(results_train)\n",
    "    results_valid_data.append(results_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f57f35ca9b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "results_train_data_all = []\n",
    "results_valid_data_all = []\n",
    "results_train_data_all = np.zeros(epoch+1)\n",
    "results_valid_data_all = np.zeros(epoch+1)\n",
    "# 目的関数の出力 (loss)\n",
    "for i in range(split_num):   \n",
    "    results_train_data_all += results_train_data[i]['loss']\n",
    "    results_valid_data_all += results_valid_data[i]['loss']\n",
    "    #plt.plot(results_train_data[i]['loss'], label='train')  # label で凡例の設定\n",
    "    #plt.plot(results_valid_data[i]['loss'], label='valid')  # label で凡例の設定\n",
    "    #plt.legend()  # 凡例の表示\n",
    "    #plt.figure()\n",
    "\n",
    "plt.plot(results_train_data_all / 10, label='train')  # label で凡例の設定\n",
    "plt.plot(results_valid_data_all / 10, label='valid')  # label で凡例の設定\n",
    "plt.legend()  # 凡例の表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.23535322 2.16941769 2.14440741 2.12920447 2.11931248 2.11298235\n",
      " 2.1088088  2.1058611  2.10366723 2.1019666  2.10055968 2.09937321\n",
      " 2.09835178 2.09745985 2.09669174 2.09601137 2.09540942 2.0948773\n",
      " 2.09440427 2.09397469 2.09359266 2.09323308 2.0929094  2.09261194\n",
      " 2.0923346  2.09207983 2.09183899 2.09161651 2.09140769 2.09120737\n",
      " 2.09101495 2.09082516 2.09064701 2.09048007 2.09031411 2.09015957\n",
      " 2.09001017 2.08986399 2.08972245 2.08958869 2.08945775 2.08932942\n",
      " 2.0892092  2.08908944 2.08897673 2.08886774 2.08876173 2.08865374\n",
      " 2.0885479  2.088443   2.08834404 2.08824261 2.08814463 2.08805083\n",
      " 2.0879567  2.08786845 2.08778144 2.08769353 2.08760576 2.08752215\n",
      " 2.0874356  2.087355   2.08727506 2.08719376 2.08711712 2.08704042\n",
      " 2.08696543 2.08689326 2.08682288 2.08675259 2.08668332 2.08661352\n",
      " 2.08654399 2.08647634 2.08640859 2.08633997 2.08627218 2.08620565\n",
      " 2.08614154 2.0860807  2.08601677 2.08595604 2.08589441 2.08583617\n",
      " 2.08577661 2.08571735 2.0856587  2.08560114 2.08554475 2.08548813\n",
      " 2.08543172 2.08537568 2.0853184  2.08526452 2.08520994 2.08515659\n",
      " 2.08510448 2.08505058 2.08499694 2.08494768]\n",
      "[2.19030869 2.15597366 2.13687713 2.12469234 2.11705668 2.11218801\n",
      " 2.10885073 2.10642029 2.1046482  2.10322963 2.10208474 2.10112396\n",
      " 2.100334   2.0996428  2.09904106 2.09850004 2.09800412 2.0975887\n",
      " 2.09720717 2.09687242 2.09658171 2.09630564 2.09603705 2.09578815\n",
      " 2.09555727 2.09533302 2.09513596 2.09494419 2.09476011 2.09459558\n",
      " 2.09443481 2.09427941 2.09414813 2.09403035 2.0939043  2.09377804\n",
      " 2.09365906 2.09355609 2.09346804 2.09337746 2.09329593 2.09321347\n",
      " 2.09313886 2.09306464 2.09299549 2.09292921 2.09286803 2.09280902\n",
      " 2.09274313 2.09268679 2.09263895 2.09258988 2.09254083 2.09250446\n",
      " 2.0924712  2.09243941 2.09240653 2.09237833 2.09234226 2.09230722\n",
      " 2.09226727 2.09223707 2.09220901 2.09218368 2.0921537  2.09214056\n",
      " 2.09211242 2.09208515 2.09206359 2.09204273 2.092011   2.09199369\n",
      " 2.09196959 2.09194547 2.09191839 2.0918847  2.09186277 2.0918377\n",
      " 2.09180331 2.09177208 2.09173948 2.09171498 2.0916793  2.0916438\n",
      " 2.09161276 2.09157531 2.09154068 2.09152094 2.0914862  2.09145005\n",
      " 2.09142385 2.09139876 2.09137665 2.09134784 2.09132513 2.0913005\n",
      " 2.09126928 2.09124409 2.09122813 2.09119925]\n"
     ]
    }
   ],
   "source": [
    "print(results_train_data_all)\n",
    "print(results_valid_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
